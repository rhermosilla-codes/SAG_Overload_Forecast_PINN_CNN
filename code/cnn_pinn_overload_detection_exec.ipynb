{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930549a6ebc6d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries import\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Input, Conv2D, BatchNormalization, LeakyReLU, AveragePooling2D, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
    "from sklearn.metrics import f1_score as f1_s\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import gc\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b31df43ae9c9a",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d6e3ae2d9f12dee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:32.016740Z",
     "start_time": "2024-06-20T22:30:31.669989Z"
    }
   },
   "outputs": [],
   "source": [
    "#loading data from source\n",
    "data = np.load('../data/matrices.npz')\n",
    "X = data['matriz_a']  #X values \n",
    "y = data['matriz_b']  #y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ef9a418fa4d8f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:32.054983Z",
     "start_time": "2024-06-20T22:30:32.018184Z"
    }
   },
   "outputs": [],
   "source": [
    "#Split of data in a train, validation, and test sets. Observe the use of non-random to care for the time-series overlapping.\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.15, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda955ddbd7bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step to convert each set in valid tensors.\n",
    "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "X_val = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
    "y_val = tf.convert_to_tensor(y_val, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c323fd99122ecd9c",
   "metadata": {},
   "source": [
    "### PINNs definition section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b3de10b7962f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:32.431362Z",
     "start_time": "2024-06-20T22:30:32.420705Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a set of trainable parameters for each physics rule.\n",
    "# These lists contain trainable TensorFlow variables (l1 and l2) for 12 different physics rules.\n",
    "# Each variable is initialized to 1.0 and is of type float32.\n",
    "# The 'trainable=True' argument ensures that these variables will be optimized during the training process.\n",
    "# The 'name' argument assigns a unique name to each variable, using the format \"l1_i\" and \"l2_i\",\n",
    "# where 'i' is the index of the physics rule.\n",
    "l1 = [tf.Variable(initial_value=1.0, dtype=tf.float32, trainable=True, name=f\"l1_{i}\") for i in range(12)]\n",
    "l2 = [tf.Variable(initial_value=1.0, dtype=tf.float32, trainable=True, name=f\"l2_{i}\") for i in range(12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97c6ce1965a6bb69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:32.434775Z",
     "start_time": "2024-06-20T22:30:32.432654Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39e9a50f260cca9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:32.439011Z",
     "start_time": "2024-06-20T22:30:32.436016Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the sigmoid activation function.\n",
    "# This function computes the sigmoid of the input tensor 'z'.\n",
    "# The sigmoid function is given by the formula: 1 / (1 + exp(-z)).\n",
    "# It maps any real-valued number into the range (0, 1).\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    z (tensor): Input tensor.\n",
    "    Returns:\n",
    "    tensor: The sigmoid of the input tensor.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + tf.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec46181da06b973b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:32.444109Z",
     "start_time": "2024-06-20T22:30:32.440295Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the normalize function.\n",
    "# This function scales the input pairs (alpha, beta) based on the maximum absolute value\n",
    "# among the elements in each pair. The purpose of this function is to normalize\n",
    "# the inputs to a common scale to improve numerical stability during computation.\n",
    "# The maximum absolute value between x and y is computed and used as the scaling factor.\n",
    "# Additionally, a small constant epsilon is added to avoid division by zero.\n",
    "def normalize(alpha, beta):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    alpha (tensor): First input tensor.\n",
    "    beta (tensor): Second input tensor.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the normalized tensors (x, y).\n",
    "    \"\"\"\n",
    "    max_value = tf.maximum(tf.abs(alpha), tf.abs(beta))\n",
    "    return alpha / (max_value + tf.keras.backend.epsilon()), beta / (max_value + tf.keras.backend.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28e86d6b2ee3b05f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:32.449114Z",
     "start_time": "2024-06-20T22:30:32.445558Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the slope_penalizer function.\n",
    "# This function penalizes the slopes of the input pairs (alpha, beta).\n",
    "# It first normalizes the input pairs using the normalize function.\n",
    "# Then, it computes the penalization condition based on the absolute values of the\n",
    "# trainable parameters (l1[i] and l2[i]) and applies the sigmoid function to these values.\n",
    "# The penalization condition is rounded to obtain binary values (0 or 1).\n",
    "# Finally, the penalization value is determined based on the penalization condition:\n",
    "# if the condition is greater than 0, the penalization value is set to 1.0, otherwise to 0.0.\n",
    "def slope_penalizer(alpha, beta, i):\n",
    "    x_norm, y_norm = normalize(alpha, beta)\n",
    "    penalization_condition = sigmoid(-tf.abs(l1[i]) * x_norm) * sigmoid(tf.abs(l2[i]) * y_norm)\n",
    "    penalization_condition = tf.round(penalization_condition)\n",
    "    penalization_value = tf.where(penalization_condition > 0, 1.0, 0.0)\n",
    "    return penalization_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff714e71-cde1-41a9-9c32-e8716d9fa016",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:32.457382Z",
     "start_time": "2024-06-20T22:30:32.453272Z"
    }
   },
   "outputs": [],
   "source": [
    "class PhysicRules(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(PhysicRules, self).__init__()\n",
    "\n",
    "    # Define the forward pass (call method) for the Physics loss function.\n",
    "    # This method takes in the inputs alpha, beta, and y_pred.\n",
    "    # It applies a series of penalizer terms calculated by the slope_penalizer function\n",
    "    # for each pair (alpha[i], beta[i]).\n",
    "    # The maximum penalizer term across all pairs is computed and expanded to match the shape of y_pred.\n",
    "    # The y_pred tensor is binarized using a threshold of 0.5.\n",
    "    # The final loss is calculated as the product of the maximum penalizer term and the binarized y_pred.\n",
    "    def call(self, alpha, beta, y_pred):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        alpha (tensor): First input tensor.\n",
    "        beta (tensor): Second input tensor.\n",
    "        y_pred (tensor): Predicted output tensor.\n",
    "        \n",
    "        Returns:\n",
    "        tensor: The calculated physics-informed loss.\n",
    "        \"\"\"\n",
    "        penalized_terms = [slope_penalizer(alpha[i], beta[i], i) for i in range(len(alpha))]\n",
    "        reg_max = tf.reduce_max(tf.stack(penalized_terms, axis=0), axis=0)\n",
    "        reg_max = tf.expand_dims(reg_max, axis=-1)  # Expandir dims para hacer compatible con y_pred\n",
    "        y_pred = tf.where(y_pred >= 0.5, 1.0, 0.0)\n",
    "        loss_ = reg_max * y_pred\n",
    "        return loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5589f09664d1b970",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:32.462712Z",
     "start_time": "2024-06-20T22:30:32.458931Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the calculate_slope function.\n",
    "# This function calculates the mean slopes of the input tensor over time.\n",
    "# Inputs:\n",
    "# - inputs: A tensor of shape (batch_size, timesteps, features).\n",
    "# The function computes the differences between consecutive timesteps,\n",
    "# then divides these differences by the respective time intervals.\n",
    "# Finally, it averages the slopes over time for each feature and each sample in the batch.\n",
    "def calculate_slope(inputs):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    inputs (tensor): Input tensor of shape (batch_size, timesteps, features).\n",
    "    \n",
    "    Returns:\n",
    "    tensor: The mean slopes calculated from the input tensor over time.\n",
    "    \"\"\"\n",
    "    timesteps = tf.shape(inputs)[1]\n",
    "    diffs = inputs[:, 1:, :] - inputs[:, :-1, :]\n",
    "    time_intervals = tf.range(1, timesteps, dtype=tf.float32)\n",
    "    time_intervals = tf.reshape(time_intervals, (1, -1, 1))\n",
    "    slopes = diffs / time_intervals\n",
    "    mean_slopes = tf.reduce_mean(slopes, axis=1)\n",
    "    return mean_slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b87e6c3a-ba17-411a-98fe-dd035c64d070",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:32.469208Z",
     "start_time": "2024-06-20T22:30:32.464148Z"
    }
   },
   "outputs": [],
   "source": [
    "physic_rules = PhysicRules()  # Create an instance of the PhysicRules class to use its methods and encapsulate its state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca90e80f-d9ba-4c16-af11-9fe0a3e6a2ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:32.478423Z",
     "start_time": "2024-06-20T22:30:32.470781Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the PhysicLoss class, which extends the tf.keras.losses.Loss class.\n",
    "# This class implements a custom loss function for a neural network incorporating\n",
    "# physics-informed constraints. The loss function combines a binary cross-entropy\n",
    "# loss with a physics-based penalty term.\n",
    "class PhysicLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, name=\"SAGLoss\", lambda_factor=1):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        name (str): Name of the loss function.\n",
    "        lambda_factor (float): Factor to scale the physics-based penalty term.\n",
    "        \"\"\"\n",
    "        super(PhysicLoss, self).__init__(name=name)\n",
    "        self.bce = BinaryFocalCrossentropy()\n",
    "        self.calculate_slope = calculate_slope\n",
    "        self.lambda_factor = lambda_factor\n",
    "\n",
    "    # Calculate the gradients (slopes) of the input tensor.\n",
    "    # The gradients are used to determine the physics-based penalty terms.\n",
    "    def getGradients(self, inputs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        inputs (tensor): Input data for the model.\n",
    "        \n",
    "        Returns:\n",
    "        tuple: A tuple containing lists of alpha and beta gradients.\n",
    "        \"\"\"\n",
    "        gradients = self.calculate_slope(inputs)\n",
    "        alphas = [\n",
    "            gradients[:, 4],\n",
    "            gradients[:, 6],\n",
    "            gradients[:, 0],\n",
    "            gradients[:, 3],\n",
    "            gradients[:, 0],\n",
    "            gradients[:, 1],\n",
    "            gradients[:, 4],\n",
    "            gradients[:, 2]\n",
    "        ]\n",
    "        betas = [\n",
    "            gradients[:, 3],\n",
    "            gradients[:, 3],\n",
    "            gradients[:, 1],\n",
    "            gradients[:, 7],\n",
    "            gradients[:, 2],\n",
    "            gradients[:, 2],\n",
    "            gradients[:, 7],\n",
    "            gradients[:, 4]\n",
    "        ]\n",
    "        rules = [\n",
    "            [1, 1],  #up & down (default relation) 1\n",
    "            [1, 1],  #up & down 2\n",
    "            [-1, -1],  #down & up 3\n",
    "            [-1, -1],  #down & up 4\n",
    "            [-1, 1],  #down & down 5\n",
    "            [1, 1],  #up & down 6\n",
    "            [1, -1],  #up & up 7\n",
    "            [1, -1]  #up & up 8\n",
    "        ]\n",
    "        for i in range(len(rules)):\n",
    "            alphas[i] *= rules[i][0]\n",
    "            betas[i] *= rules[i][1]\n",
    "\n",
    "        return alphas, betas\n",
    "\n",
    "    # Calculate the physics-based loss.\n",
    "    # This loss penalizes violations of the physics-based constraints.\n",
    "    def getPhysicsLoss(self, inputs, y_pred):\n",
    "        \"\"\"\n",
    "         Parameters:\n",
    "         inputs (tensor): Input data for the model.\n",
    "         y_pred (tensor): Predicted output tensor.\n",
    "        \n",
    "         Returns:\n",
    "         tensor: The calculated physics-based loss.\n",
    "         \"\"\"\n",
    "        alphas, betas = self.getGradients(inputs)\n",
    "        penalized_max_loss_terms = physic_rules(alphas, betas, y_pred)\n",
    "        penalized_max_loss_terms = tf.reduce_mean(penalized_max_loss_terms)\n",
    "        return penalized_max_loss_terms\n",
    "\n",
    "    # Return all components of the loss for logging or analysis.\n",
    "    def getAllLosses(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        tuple: A tuple containing the total loss, focal loss, and physics loss.\n",
    "        \"\"\"\n",
    "        return self.total_loss, self.focal_loss, self.physic_loss\n",
    "\n",
    "    # Calculate the total loss, which is a combination of the focal loss and the physics-based loss.\n",
    "    def getTotalLoss(self, inputs, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        inputs (tensor): Input data for the model.\n",
    "        y_true (tensor): True labels corresponding to the input data.\n",
    "        y_pred (tensor): Predicted output tensor.\n",
    "        \n",
    "        Returns:\n",
    "        tensor: The total loss for this training step.\n",
    "        \"\"\"\n",
    "        focal_loss = self.bce(y_true, y_pred)\n",
    "        physic_loss = self.getPhysicsLoss(inputs, y_pred)\n",
    "        total_loss = focal_loss + self.lambda_factor * physic_loss\n",
    "        self.total_loss = total_loss\n",
    "        self.focal_loss = focal_loss\n",
    "        self.physic_loss = physic_loss\n",
    "        return total_loss\n",
    "\n",
    "    # Define the call method, which computes the total loss given the actual data and the predictions.\n",
    "    def call(self, actual_data, y_pred):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        actual_data (tuple): A tuple containing the input data and the true labels.\n",
    "        y_pred (tensor): Predicted output tensor.\n",
    "        \n",
    "        Returns:\n",
    "        tensor: The total loss for this training step.\n",
    "        \"\"\"\n",
    "        inputs, y_true = actual_data\n",
    "        total_loss = self.getTotalLoss(inputs, y_true, y_pred)\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5984a48b959a74c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:32.482597Z",
     "start_time": "2024-06-20T22:30:32.479526Z"
    }
   },
   "outputs": [],
   "source": [
    "pinn_loss = PhysicLoss(\"PhysicLoss\")  # Create an instance of the PhysicLoss class to use its custom loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aed1f260d19df6a",
   "metadata": {},
   "source": [
    "### Gramm matrices definition section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "773450894701a823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:32.489432Z",
     "start_time": "2024-06-20T22:30:32.484027Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the CalculateGramMatrix class, which extends the tf.keras.layers.Layer class.\n",
    "# This class computes the Gram matrix based on the input pairs of angles.\n",
    "class CalculateGramMatrix(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        pairs_n (list): List of pairs of indices used to calculate angular differences.\n",
    "        \"\"\"\n",
    "        acceptable_kwargs = {k: v for k, v in kwargs.items() if\n",
    "                             k in ['name', 'trainable', 'dtype', 'dynamic', 'input_shape']}\n",
    "        super(CalculateGramMatrix, self).__init__(**acceptable_kwargs)\n",
    "        self.pairs_n = kwargs.get('pairs_n',\n",
    "                                  [[4, 3], [6, 4], [6, 3], [3, 5], [0, 1], [3, 7], [5, 7], [0, 2], [1, 2], [4, 7],\n",
    "                                   [2, 4]])\n",
    "\n",
    "    # Define the forward pass (call method) for the CalculateGramMatrix layer.\n",
    "    # This method takes in the inputs and computes the Gram matrix based on angular differences.\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        inputs (tensor): Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "        tensor: The Gram matrix calculated from the angular differences.\n",
    "        \"\"\"\n",
    "        angles = tf.math.acos(inputs)\n",
    "\n",
    "        matrix = []\n",
    "        for i, j in self.pairs_n:\n",
    "            angles_j1 = angles[:, :, i]  # Angles of the first feature in the pair.\n",
    "            angles_j2 = angles[:, :, j]  # Angles of the second feature in the pair.\n",
    "\n",
    "            #expand the matrices to allow the next add operations\n",
    "            angles_j1_expanded = tf.expand_dims(angles_j1, axis=2)\n",
    "            angles_j2_expanded = tf.expand_dims(angles_j2, axis=1)\n",
    "\n",
    "            # Calculate the angular difference matrices (30x30)\n",
    "            angular_difference_matrix = angles_j1_expanded - angles_j2_expanded\n",
    "            matrix.append(angular_difference_matrix)\n",
    "        m = tf.stack(matrix, axis=-1)\n",
    "        return m\n",
    "\n",
    "    # Define the get_config method to serialize the layer configuration.\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        dict: Configuration dictionary for serializing the layer.\n",
    "        \"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update({'pairs_n': self.pairs_n})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23fc095565c78a1",
   "metadata": {},
   "source": [
    "### Temporal filter definition section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e66467ed8a39f1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:32.496132Z",
     "start_time": "2024-06-20T22:30:32.490750Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the CustomTemporalFilter class, which extends the tf.keras.layers.Layer class.\n",
    "# This class applies a custom temporal filter to the input tensor.\n",
    "class CustomTemporalFilter(Layer):\n",
    "    def __init__(self, filter_size, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        filter_size (int): Size of the filter to be applied.\n",
    "        \"\"\"\n",
    "        super(CustomTemporalFilter, self).__init__(**kwargs)\n",
    "        self.filter_size = filter_size\n",
    "\n",
    "    # Define the get_config method to serialize the layer configuration.\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        dict: Configuration dictionary for serializing the layer.\n",
    "        \"\"\"\n",
    "        config = super(CustomTemporalFilter, self).get_config()\n",
    "        config.update({\n",
    "            \"filter_size\": self.filter_size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    # Build the layer by creating an filter that decreases from the bottom-right\n",
    "    # corner to the top-left corner.\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        input_shape (tensor): Shape of the input tensor.\n",
    "        \"\"\"\n",
    "        # Create a 2D matrix where each element is the value of its normalized index.\n",
    "        # This creates a gradient that decreases towards the top-left corner.\n",
    "        x = tf.linspace(1.0, 0.0, self.filter_size)\n",
    "        y = tf.linspace(1.0, 0.0, self.filter_size)\n",
    "        X, Y = tf.meshgrid(x, y)\n",
    "        self.filter = 1.0 - ((X + Y) / 2.0)  # Normalize to have values from 0 to 1\n",
    "        self.filter = tf.reshape(self.filter, (1, self.filter_size, self.filter_size, 1))\n",
    "        self.filter = tf.cast(self.filter, dtype='float32')\n",
    "\n",
    "    # Define the forward pass (call method) for the CustomTemporalFilter layer.\n",
    "    # This method applies the custom temporal filter to the input tensor.\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        inputs (tensor): Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "        tensor: The input tensor after applying the custom temporal filter.\n",
    "        \"\"\"\n",
    "        # Adjust the filter to match the batch size and number of channels of the inputs.\n",
    "        filter_broadcasted = tf.tile(self.filter, [tf.shape(inputs)[0], 1, 1, tf.shape(inputs)[-1]])\n",
    "\n",
    "        # Apply the filter to the inputs.\n",
    "        return inputs * filter_broadcasted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98df7880786d109",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50aa5dd6361d7fb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:32.502413Z",
     "start_time": "2024-06-20T22:30:32.497404Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the function to create the model.\n",
    "# This function builds a neural network model with specified numbers of convolutional and dense layers.\n",
    "# The model includes a CalculateGramMatrix layer, a CustomTemporalFilter layer,\n",
    "# and dynamically adds convolutional and dense layers based on the given parameters.\n",
    "def create_model(num_conv_layers, num_dense_layers, num_neurons, dropout_rate, conv_filters):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    num_conv_layers (int): Number of convolutional layers to add.\n",
    "    num_dense_layers (int): Number of dense layers to add.\n",
    "    num_neurons (int): Number of neurons in the first dense layer.\n",
    "    dropout_rate (float): Dropout rate to apply after each dense layer.\n",
    "    conv_filters (int): Number of filters in the first convolutional layer.\n",
    "    \n",
    "    Returns:\n",
    "    tf.keras.Model: The constructed neural network model.\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(30, 8), name=\"Input\")\n",
    "    m = CalculateGramMatrix(name=\"Gram_converter\")(input_layer)\n",
    "    m = CustomTemporalFilter(filter_size=30, name=\"Temporal_filter\")(m)\n",
    "\n",
    "    # Add convolutional layers dynamically\n",
    "    for i in range(num_conv_layers):\n",
    "        print(f'conv_filters: {conv_filters * (2 ** i)}')\n",
    "        m = Conv2D(filters=conv_filters * (2 ** i), kernel_size=(3, 3), use_bias=False, kernel_initializer='he_normal')(\n",
    "            m)\n",
    "        m = BatchNormalization()(m)\n",
    "        m = LeakyReLU(alpha=0.01)(m)\n",
    "        m = AveragePooling2D(pool_size=(2, 2))(m)\n",
    "\n",
    "    c = Flatten(name=\"Flattened_after_full\")(m)\n",
    "\n",
    "    # Add dense layers dynamically\n",
    "    for j in range(num_dense_layers):\n",
    "        print(f'num_neurons: {num_neurons // (2 ** j)}')\n",
    "        c = Dense(num_neurons // (2 ** j), activation='relu', kernel_initializer='he_normal')(c)\n",
    "        c = Dropout(dropout_rate)(c)\n",
    "        c = BatchNormalization()(c)\n",
    "    output_layer = Dense(1, activation='sigmoid', name=\"Output\")(c)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6160afd6d36cae8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:32.506955Z",
     "start_time": "2024-06-20T22:30:32.503776Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the function to delete the model and clear the session.\n",
    "# This function clears the current TensorFlow session, deletes the model and optimizer variables,\n",
    "# and performs garbage collection to free up memory.\n",
    "def del_model():\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    None\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "    try:\n",
    "        del model\n",
    "        del optimizer\n",
    "    except:\n",
    "        None\n",
    "    for i in range(15):\n",
    "        gc.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df8c93bf695b599f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:32.513044Z",
     "start_time": "2024-06-20T22:30:32.508464Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the F1Score class, which extends the tf.keras.metrics.Metric class.\n",
    "# This class calculates the F1 score, which is the harmonic mean of precision and recall.\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        \"\"\"        \n",
    "        Parameters:\n",
    "        name (str): Name of the metric.\n",
    "        kwargs (dict): Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = Precision()\n",
    "        self.recall = Recall()\n",
    "\n",
    "    # Update the state of the metric with the true and predicted values.\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"        \n",
    "        Parameters:\n",
    "        y_true (tensor): True labels.\n",
    "        y_pred (tensor): Predicted labels.\n",
    "        sample_weight (tensor, optional): Optional weighting of each example.\n",
    "        \n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    # Compute the result of the metric.\n",
    "    def result(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        tensor: The computed F1 score.\n",
    "        \"\"\"\n",
    "        precision = self.precision.result()\n",
    "        recall = self.recall.result()\n",
    "        return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n",
    "\n",
    "    # Reset the states of the precision and recall metrics.\n",
    "    def reset_states(self):\n",
    "        \"\"\"  \n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "446b6348-62fc-4934-8876-3f5f6e07a293",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:32.518185Z",
     "start_time": "2024-06-20T22:30:32.514397Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform a single training step.\n",
    "# This function computes the model predictions, calculates the custom PINN loss,\n",
    "# computes gradients, and applies these gradients to update the model's weights.\n",
    "@tf.function\n",
    "def train_step(inputs, y_true, model, optimizer):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    inputs (tensor): Input data for the model.\n",
    "    y_true (tensor): True labels corresponding to the input data.\n",
    "    model (tf.keras.Model): The model to be trained.\n",
    "    optimizer (tf.keras.optimizers.Optimizer): The optimizer to use for updating the model's weights.\n",
    "    \n",
    "    Returns:\n",
    "    tensor: The total loss for this training step.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(inputs, training=True)\n",
    "        total_loss = pinn_loss((inputs, y_true), y_pred)  # Compute the custom PINN loss\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)  # Calculate gradients\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))  # Apply gradients to update model weights\n",
    "    return total_loss  # Return the total loss for this training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1100e1e7-2481-468f-b4eb-048825e72b8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:32.522321Z",
     "start_time": "2024-06-20T22:30:32.519431Z"
    }
   },
   "outputs": [],
   "source": [
    "# These parameters are the result of a grid search.\n",
    "# The grid search was conducted to find the optimal hyperparameters\n",
    "# for the neural network model.\n",
    "params = {\n",
    "    'num_conv_layers': 2,\n",
    "    'num_dense_layers': 3,\n",
    "    'num_neurons': 1024,\n",
    "    'dropout_rate': 0.3,\n",
    "    'lambda_factor': 1.0,  # Put in 0.0 to deactivate PINN loss calculation\n",
    "    'conv_filters': 64  # Different base numbers of filters for the convolutional layers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a215f847-7960-48b0-aa7b-1d3741ba5fa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:33.254089Z",
     "start_time": "2024-06-20T22:30:32.523606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_filters: 64\n",
      "conv_filters: 128\n",
      "num_neurons: 1024\n",
      "num_neurons: 512\n",
      "num_neurons: 256\n"
     ]
    }
   ],
   "source": [
    "model = create_model(\n",
    "    num_conv_layers=params['num_conv_layers'],\n",
    "    num_dense_layers=params['num_dense_layers'],\n",
    "    num_neurons=params['num_neurons'],\n",
    "    dropout_rate=params['dropout_rate'],\n",
    "    conv_filters=params['conv_filters']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb4e5f51-ea9a-4c6e-95a3-896887c5927a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:33.267519Z",
     "start_time": "2024-06-20T22:30:33.255682Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the evaluation metrics and loss function.\n",
    "# These metrics and loss function will be used to assess the performance of the model.\n",
    "\n",
    "precision_metric = Precision()  # Precision metric to measure the proportion of true positives among all positive predictions\n",
    "recall_metric = Recall()  # Recall metric to measure the proportion of true positives among all actual positives\n",
    "bce = BinaryFocalCrossentropy()  # Binary Focal Crossentropy loss function to handle class imbalance\n",
    "f1_metric = F1Score()  # F1 Score metric to combine precision and recall into a single measure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7421c857f3581d7",
   "metadata": {},
   "source": [
    "### Training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e4c67d5-c4be-4751-9d02-bb0acc86b78d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:33.274616Z",
     "start_time": "2024-06-20T22:30:33.268893Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the training parameters.\n",
    "patience = 200  # Number of epochs with no improvement after which training will be stopped\n",
    "epochs = 5000  # Total number of epochs to train the model\n",
    "cont_patience = 0  # Counter to keep track of epochs with no improvement\n",
    "min_patience = 0  # Variable to store the minimum patience value observed\n",
    "best_model = None  # Variable to store the best model observed during training\n",
    "batch_size = 2048  # Size of the mini-batches used during training\n",
    "del_model()  # Clear any existing models and free up memory\n",
    "lambda_factor = params['lambda_factor']  # Retrieve the lambda factor from the parameters dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "673f2b0c-5223-4128-b16a-c431c91a31da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:30:33.314676Z",
     "start_time": "2024-06-20T22:30:33.276093Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the optimizer and build it with the model's trainable variables.\n",
    "optimizer = tf.keras.optimizers.Adam()  # Use Adam optimizer for training\n",
    "optimizer.build(model.trainable_variables)  # Build the optimizer with the trainable variables of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c18e95b8-a43c-464b-ac14-90462e115914",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:41:03.411850Z",
     "start_time": "2024-06-20T22:30:33.316212Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 18:30:34.948129: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2024-06-20 18:30:36.797887: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f59ad69c690 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-20 18:30:36.797919: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Ti, Compute Capability 8.6\n",
      "2024-06-20 18:30:36.801618: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-06-20 18:30:36.863818: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, IMPROVED <total_loss: 0.1115> <bce_loss: 0.1115> <physics_loss: 0.0000> 0.0000->0.0000 (1)     \n",
      "Epoch 1, IMPROVED <total_loss: 0.1237> <bce_loss: 0.1237> <physics_loss: 0.0000> 0.0000->0.0000 (1)     \n",
      "Epoch 2, IMPROVED <total_loss: 0.0781> <bce_loss: 0.0776> <physics_loss: 0.0005> 0.0000->0.0348 (1)     \n",
      "Epoch 3, IMPROVED <total_loss: 0.0692> <bce_loss: 0.0676> <physics_loss: 0.0016> 0.0348->0.1160 (1)     \n",
      "Epoch 4, NOT IMPROVED <total_loss: 0.0667> <bce_loss: 0.0659> <physics_loss: 0.0008> 0.1160->0.0907 (1)    \n",
      "Epoch 5, IMPROVED <total_loss: 0.0663> <bce_loss: 0.0652> <physics_loss: 0.0011> 0.1160->0.1219 (2)     \n",
      "Epoch 6, IMPROVED <total_loss: 0.0644> <bce_loss: 0.0633> <physics_loss: 0.0011> 0.1219->0.1271 (1)     \n",
      "Epoch 7, IMPROVED <total_loss: 0.0592> <bce_loss: 0.0546> <physics_loss: 0.0046> 0.1271->0.3128 (1)     \n",
      "Epoch 8, IMPROVED <total_loss: 0.0563> <bce_loss: 0.0509> <physics_loss: 0.0054> 0.3128->0.3404 (1)     \n",
      "Epoch 9, IMPROVED <total_loss: 0.0576> <bce_loss: 0.0506> <physics_loss: 0.0070> 0.3404->0.3991 (1)     \n",
      "Epoch 10, IMPROVED <total_loss: 0.0579> <bce_loss: 0.0484> <physics_loss: 0.0095> 0.3991->0.4394 (1)     \n",
      "Epoch 11, NOT IMPROVED <total_loss: 0.0539> <bce_loss: 0.0496> <physics_loss: 0.0043> 0.4394->0.3593 (1)    \n",
      "Epoch 12, IMPROVED <total_loss: 0.0532> <bce_loss: 0.0426> <physics_loss: 0.0106> 0.4394->0.5560 (2)     \n",
      "Epoch 13, NOT IMPROVED <total_loss: 0.0503> <bce_loss: 0.0440> <physics_loss: 0.0062> 0.5560->0.4568 (1)    \n",
      "Epoch 14, NOT IMPROVED <total_loss: 0.0489> <bce_loss: 0.0418> <physics_loss: 0.0070> 0.5560->0.4760 (2)    \n",
      "Epoch 15, NOT IMPROVED <total_loss: 0.0477> <bce_loss: 0.0396> <physics_loss: 0.0081> 0.5560->0.5285 (3)    \n",
      "Epoch 16, IMPROVED <total_loss: 0.0471> <bce_loss: 0.0387> <physics_loss: 0.0084> 0.5560->0.5749 (4)     \n",
      "Epoch 17, NOT IMPROVED <total_loss: 0.0461> <bce_loss: 0.0377> <physics_loss: 0.0084> 0.5749->0.5621 (1)    \n",
      "Epoch 18, IMPROVED <total_loss: 0.0423> <bce_loss: 0.0331> <physics_loss: 0.0092> 0.5749->0.6513 (2)     \n",
      "Epoch 19, IMPROVED <total_loss: 0.0464> <bce_loss: 0.0331> <physics_loss: 0.0133> 0.6513->0.6806 (1)     \n",
      "Epoch 20, NOT IMPROVED <total_loss: 0.0454> <bce_loss: 0.0368> <physics_loss: 0.0087> 0.6806->0.5980 (1)    \n",
      "Epoch 21, IMPROVED <total_loss: 0.0437> <bce_loss: 0.0296> <physics_loss: 0.0141> 0.6806->0.7756 (2)     \n",
      "Epoch 22, NOT IMPROVED <total_loss: 0.0418> <bce_loss: 0.0302> <physics_loss: 0.0116> 0.7756->0.6924 (1)    \n",
      "Epoch 23, NOT IMPROVED <total_loss: 0.0446> <bce_loss: 0.0335> <physics_loss: 0.0111> 0.7756->0.6898 (2)    \n",
      "Epoch 24, IMPROVED <total_loss: 0.0413> <bce_loss: 0.0269> <physics_loss: 0.0143> 0.7756->0.8000 (3)     \n",
      "Epoch 25, IMPROVED <total_loss: 0.0392> <bce_loss: 0.0257> <physics_loss: 0.0135> 0.8000->0.8020 (1)     \n",
      "Epoch 26, NOT IMPROVED <total_loss: 0.0429> <bce_loss: 0.0327> <physics_loss: 0.0103> 0.8020->0.7111 (1)    \n",
      "Epoch 27, IMPROVED <total_loss: 0.0383> <bce_loss: 0.0223> <physics_loss: 0.0160> 0.8020->0.8408 (2)     \n",
      "Epoch 28, IMPROVED <total_loss: 0.0378> <bce_loss: 0.0221> <physics_loss: 0.0157> 0.8408->0.8567 (1)     \n",
      "Epoch 29, NOT IMPROVED <total_loss: 0.0378> <bce_loss: 0.0224> <physics_loss: 0.0154> 0.8567->0.8470 (1)    \n",
      "Epoch 30, NOT IMPROVED <total_loss: 0.0409> <bce_loss: 0.0276> <physics_loss: 0.0133> 0.8567->0.7811 (2)    \n",
      "Epoch 31, NOT IMPROVED <total_loss: 0.0392> <bce_loss: 0.0248> <physics_loss: 0.0143> 0.8567->0.8221 (3)    \n",
      "Epoch 32, NOT IMPROVED <total_loss: 0.0426> <bce_loss: 0.0317> <physics_loss: 0.0108> 0.8567->0.7545 (4)    \n",
      "Epoch 33, NOT IMPROVED <total_loss: 0.0391> <bce_loss: 0.0248> <physics_loss: 0.0143> 0.8567->0.8361 (5)    \n",
      "Epoch 34, IMPROVED <total_loss: 0.0400> <bce_loss: 0.0227> <physics_loss: 0.0173> 0.8567->0.8727 (6)     \n",
      "Epoch 35, NOT IMPROVED <total_loss: 0.0378> <bce_loss: 0.0199> <physics_loss: 0.0179> 0.8727->0.8716 (1)    \n",
      "Epoch 36, IMPROVED <total_loss: 0.0368> <bce_loss: 0.0200> <physics_loss: 0.0168> 0.8727->0.8846 (2)     \n",
      "Epoch 37, NOT IMPROVED <total_loss: 0.0381> <bce_loss: 0.0225> <physics_loss: 0.0157> 0.8846->0.8562 (1)    \n",
      "Epoch 38, NOT IMPROVED <total_loss: 0.0383> <bce_loss: 0.0232> <physics_loss: 0.0152> 0.8846->0.8381 (2)    \n",
      "Epoch 39, NOT IMPROVED <total_loss: 0.0404> <bce_loss: 0.0217> <physics_loss: 0.0187> 0.8846->0.8772 (3)    \n",
      "Epoch 40, NOT IMPROVED <total_loss: 0.0383> <bce_loss: 0.0226> <physics_loss: 0.0157> 0.8846->0.8774 (4)    \n",
      "Epoch 41, IMPROVED <total_loss: 0.0372> <bce_loss: 0.0188> <physics_loss: 0.0184> 0.8846->0.8999 (5)     \n",
      "Epoch 42, NOT IMPROVED <total_loss: 0.0372> <bce_loss: 0.0202> <physics_loss: 0.0170> 0.8999->0.8903 (1)    \n",
      "Epoch 43, NOT IMPROVED <total_loss: 0.0372> <bce_loss: 0.0204> <physics_loss: 0.0168> 0.8999->0.8943 (2)    \n",
      "Epoch 44, NOT IMPROVED <total_loss: 0.0367> <bce_loss: 0.0199> <physics_loss: 0.0168> 0.8999->0.8958 (3)    \n",
      "Epoch 45, NOT IMPROVED <total_loss: 0.0371> <bce_loss: 0.0214> <physics_loss: 0.0157> 0.8999->0.8812 (4)    \n",
      "Epoch 46, NOT IMPROVED <total_loss: 0.0407> <bce_loss: 0.0234> <physics_loss: 0.0173> 0.8999->0.8952 (5)    \n",
      "Epoch 47, NOT IMPROVED <total_loss: 0.0391> <bce_loss: 0.0209> <physics_loss: 0.0181> 0.8999->0.8968 (6)    \n",
      "Epoch 48, IMPROVED <total_loss: 0.0357> <bce_loss: 0.0186> <physics_loss: 0.0170> 0.8999->0.9015 (7)     \n",
      "Epoch 49, IMPROVED <total_loss: 0.0377> <bce_loss: 0.0204> <physics_loss: 0.0173> 0.9015->0.9077 (1)     \n",
      "Epoch 50, NOT IMPROVED <total_loss: 0.0418> <bce_loss: 0.0234> <physics_loss: 0.0184> 0.9077->0.8841 (1)    \n",
      "Epoch 51, NOT IMPROVED <total_loss: 0.0420> <bce_loss: 0.0220> <physics_loss: 0.0200> 0.9077->0.8944 (2)    \n",
      "Epoch 52, NOT IMPROVED <total_loss: 0.0367> <bce_loss: 0.0205> <physics_loss: 0.0162> 0.9077->0.9052 (3)    \n",
      "Epoch 53, NOT IMPROVED <total_loss: 0.0388> <bce_loss: 0.0215> <physics_loss: 0.0173> 0.9077->0.8933 (4)    \n",
      "Epoch 54, NOT IMPROVED <total_loss: 0.0397> <bce_loss: 0.0224> <physics_loss: 0.0173> 0.9077->0.9027 (5)    \n",
      "Epoch 55, NOT IMPROVED <total_loss: 0.0404> <bce_loss: 0.0236> <physics_loss: 0.0168> 0.9077->0.8983 (6)    \n",
      "Epoch 56, NOT IMPROVED <total_loss: 0.0380> <bce_loss: 0.0213> <physics_loss: 0.0168> 0.9077->0.9011 (7)    \n",
      "Epoch 57, IMPROVED <total_loss: 0.0356> <bce_loss: 0.0183> <physics_loss: 0.0173> 0.9077->0.9180 (8)     \n",
      "Epoch 58, IMPROVED <total_loss: 0.0368> <bce_loss: 0.0198> <physics_loss: 0.0170> 0.9180->0.9331 (1)     \n",
      "Epoch 59, NOT IMPROVED <total_loss: 0.0375> <bce_loss: 0.0210> <physics_loss: 0.0165> 0.9331->0.8986 (1)    \n",
      "Epoch 60, NOT IMPROVED <total_loss: 0.0413> <bce_loss: 0.0235> <physics_loss: 0.0179> 0.9331->0.9027 (2)    \n",
      "Epoch 61, NOT IMPROVED <total_loss: 0.0384> <bce_loss: 0.0219> <physics_loss: 0.0165> 0.9331->0.9122 (3)    \n",
      "Epoch 62, NOT IMPROVED <total_loss: 0.0377> <bce_loss: 0.0206> <physics_loss: 0.0170> 0.9331->0.9217 (4)    \n",
      "Epoch 63, NOT IMPROVED <total_loss: 0.0353> <bce_loss: 0.0182> <physics_loss: 0.0170> 0.9331->0.9233 (5)    \n",
      "Epoch 64, NOT IMPROVED <total_loss: 0.0379> <bce_loss: 0.0197> <physics_loss: 0.0181> 0.9331->0.9147 (6)    \n",
      "Epoch 65, NOT IMPROVED <total_loss: 0.0387> <bce_loss: 0.0214> <physics_loss: 0.0173> 0.9331->0.9012 (7)    \n",
      "Epoch 66, NOT IMPROVED <total_loss: 0.0381> <bce_loss: 0.0216> <physics_loss: 0.0165> 0.9331->0.9037 (8)    \n",
      "Epoch 67, NOT IMPROVED <total_loss: 0.0395> <bce_loss: 0.0227> <physics_loss: 0.0168> 0.9331->0.9189 (9)    \n",
      "Epoch 68, NOT IMPROVED <total_loss: 0.0379> <bce_loss: 0.0206> <physics_loss: 0.0173> 0.9331->0.9194 (10)    \n",
      "Epoch 69, NOT IMPROVED <total_loss: 0.0381> <bce_loss: 0.0213> <physics_loss: 0.0168> 0.9331->0.9108 (11)    \n",
      "Epoch 70, NOT IMPROVED <total_loss: 0.0379> <bce_loss: 0.0203> <physics_loss: 0.0176> 0.9331->0.9091 (12)    \n",
      "Epoch 71, NOT IMPROVED <total_loss: 0.0408> <bce_loss: 0.0235> <physics_loss: 0.0173> 0.9331->0.9127 (13)    \n",
      "Epoch 72, NOT IMPROVED <total_loss: 0.0390> <bce_loss: 0.0211> <physics_loss: 0.0179> 0.9331->0.9162 (14)    \n",
      "Epoch 73, NOT IMPROVED <total_loss: 0.0424> <bce_loss: 0.0246> <physics_loss: 0.0179> 0.9331->0.9169 (15)    \n",
      "Epoch 74, NOT IMPROVED <total_loss: 0.0377> <bce_loss: 0.0201> <physics_loss: 0.0176> 0.9331->0.9228 (16)    \n",
      "Epoch 75, NOT IMPROVED <total_loss: 0.0392> <bce_loss: 0.0216> <physics_loss: 0.0176> 0.9331->0.9080 (17)    \n",
      "Epoch 76, NOT IMPROVED <total_loss: 0.0457> <bce_loss: 0.0276> <physics_loss: 0.0181> 0.9331->0.8993 (18)    \n",
      "Epoch 77, NOT IMPROVED <total_loss: 0.0402> <bce_loss: 0.0227> <physics_loss: 0.0176> 0.9331->0.9110 (19)    \n",
      "Epoch 78, NOT IMPROVED <total_loss: 0.0461> <bce_loss: 0.0250> <physics_loss: 0.0211> 0.9331->0.9070 (20)    \n",
      "Epoch 79, NOT IMPROVED <total_loss: 0.0402> <bce_loss: 0.0215> <physics_loss: 0.0187> 0.9331->0.9136 (21)    \n",
      "Epoch 80, NOT IMPROVED <total_loss: 0.0430> <bce_loss: 0.0249> <physics_loss: 0.0181> 0.9331->0.8943 (22)    \n",
      "Epoch 81, NOT IMPROVED <total_loss: 0.0401> <bce_loss: 0.0231> <physics_loss: 0.0170> 0.9331->0.9091 (23)    \n",
      "Epoch 82, NOT IMPROVED <total_loss: 0.0416> <bce_loss: 0.0243> <physics_loss: 0.0173> 0.9331->0.8949 (24)    \n",
      "Epoch 83, NOT IMPROVED <total_loss: 0.0414> <bce_loss: 0.0252> <physics_loss: 0.0162> 0.9331->0.8992 (25)    \n",
      "Epoch 84, NOT IMPROVED <total_loss: 0.0405> <bce_loss: 0.0234> <physics_loss: 0.0170> 0.9331->0.9174 (26)    \n",
      "Epoch 85, NOT IMPROVED <total_loss: 0.0401> <bce_loss: 0.0233> <physics_loss: 0.0168> 0.9331->0.9049 (27)    \n",
      "Epoch 86, NOT IMPROVED <total_loss: 0.0396> <bce_loss: 0.0223> <physics_loss: 0.0173> 0.9331->0.9240 (28)    \n",
      "Epoch 87, NOT IMPROVED <total_loss: 0.0412> <bce_loss: 0.0228> <physics_loss: 0.0184> 0.9331->0.9145 (29)    \n",
      "Epoch 88, NOT IMPROVED <total_loss: 0.0421> <bce_loss: 0.0239> <physics_loss: 0.0181> 0.9331->0.9127 (30)    \n",
      "Epoch 89, NOT IMPROVED <total_loss: 0.0391> <bce_loss: 0.0213> <physics_loss: 0.0179> 0.9331->0.9168 (31)    \n",
      "Epoch 90, NOT IMPROVED <total_loss: 0.0414> <bce_loss: 0.0238> <physics_loss: 0.0176> 0.9331->0.9163 (32)    \n",
      "Epoch 91, NOT IMPROVED <total_loss: 0.0496> <bce_loss: 0.0299> <physics_loss: 0.0198> 0.9331->0.8921 (33)    \n",
      "Epoch 92, NOT IMPROVED <total_loss: 0.0395> <bce_loss: 0.0217> <physics_loss: 0.0179> 0.9331->0.9231 (34)    \n",
      "Epoch 93, IMPROVED <total_loss: 0.0376> <bce_loss: 0.0205> <physics_loss: 0.0170> 0.9331->0.9383 (35)     \n",
      "Epoch 94, NOT IMPROVED <total_loss: 0.0377> <bce_loss: 0.0209> <physics_loss: 0.0168> 0.9383->0.9224 (1)    \n",
      "Epoch 95, IMPROVED <total_loss: 0.0363> <bce_loss: 0.0187> <physics_loss: 0.0176> 0.9383->0.9405 (2)     \n",
      "Epoch 96, NOT IMPROVED <total_loss: 0.0389> <bce_loss: 0.0216> <physics_loss: 0.0173> 0.9405->0.9300 (1)    \n",
      "Epoch 97, NOT IMPROVED <total_loss: 0.0401> <bce_loss: 0.0223> <physics_loss: 0.0179> 0.9405->0.9247 (2)    \n",
      "Epoch 98, NOT IMPROVED <total_loss: 0.0396> <bce_loss: 0.0221> <physics_loss: 0.0176> 0.9405->0.9289 (3)    \n",
      "Epoch 99, NOT IMPROVED <total_loss: 0.0383> <bce_loss: 0.0205> <physics_loss: 0.0179> 0.9405->0.9303 (4)    \n",
      "Epoch 100, NOT IMPROVED <total_loss: 0.0437> <bce_loss: 0.0261> <physics_loss: 0.0176> 0.9405->0.9136 (5)    \n",
      "Epoch 101, NOT IMPROVED <total_loss: 0.0394> <bce_loss: 0.0229> <physics_loss: 0.0165> 0.9405->0.9256 (6)    \n",
      "Epoch 102, NOT IMPROVED <total_loss: 0.0421> <bce_loss: 0.0248> <physics_loss: 0.0173> 0.9405->0.9178 (7)    \n",
      "Epoch 103, NOT IMPROVED <total_loss: 0.0441> <bce_loss: 0.0254> <physics_loss: 0.0187> 0.9405->0.9112 (8)    \n",
      "Epoch 104, NOT IMPROVED <total_loss: 0.0453> <bce_loss: 0.0256> <physics_loss: 0.0198> 0.9405->0.9157 (9)    \n",
      "Epoch 105, NOT IMPROVED <total_loss: 0.0418> <bce_loss: 0.0242> <physics_loss: 0.0176> 0.9405->0.9179 (10)    \n",
      "Epoch 106, NOT IMPROVED <total_loss: 0.0429> <bce_loss: 0.0245> <physics_loss: 0.0184> 0.9405->0.9196 (11)    \n",
      "Epoch 107, NOT IMPROVED <total_loss: 0.0400> <bce_loss: 0.0227> <physics_loss: 0.0173> 0.9405->0.9224 (12)    \n",
      "Epoch 108, NOT IMPROVED <total_loss: 0.0416> <bce_loss: 0.0227> <physics_loss: 0.0189> 0.9405->0.9217 (13)    \n",
      "Epoch 109, NOT IMPROVED <total_loss: 0.0436> <bce_loss: 0.0250> <physics_loss: 0.0187> 0.9405->0.9288 (14)    \n",
      "Epoch 110, NOT IMPROVED <total_loss: 0.0432> <bce_loss: 0.0245> <physics_loss: 0.0187> 0.9405->0.9164 (15)    \n",
      "Epoch 111, NOT IMPROVED <total_loss: 0.0411> <bce_loss: 0.0235> <physics_loss: 0.0176> 0.9405->0.9178 (16)    \n",
      "Epoch 112, NOT IMPROVED <total_loss: 0.0395> <bce_loss: 0.0214> <physics_loss: 0.0181> 0.9405->0.9217 (17)    \n",
      "Epoch 113, NOT IMPROVED <total_loss: 0.0399> <bce_loss: 0.0223> <physics_loss: 0.0176> 0.9405->0.9228 (18)    \n",
      "Epoch 114, IMPROVED <total_loss: 0.0410> <bce_loss: 0.0234> <physics_loss: 0.0176> 0.9405->0.9403 (19)     \n",
      "Epoch 115, NOT IMPROVED <total_loss: 0.0388> <bce_loss: 0.0212> <physics_loss: 0.0176> 0.9405->0.9333 (1)    \n",
      "Epoch 116, NOT IMPROVED <total_loss: 0.0428> <bce_loss: 0.0230> <physics_loss: 0.0198> 0.9405->0.9255 (2)    \n",
      "Epoch 117, NOT IMPROVED <total_loss: 0.0403> <bce_loss: 0.0216> <physics_loss: 0.0187> 0.9405->0.9349 (3)    \n",
      "Epoch 118, NOT IMPROVED <total_loss: 0.0431> <bce_loss: 0.0241> <physics_loss: 0.0189> 0.9405->0.9343 (4)    \n",
      "Epoch 119, NOT IMPROVED <total_loss: 0.0403> <bce_loss: 0.0224> <physics_loss: 0.0179> 0.9405->0.9261 (5)    \n",
      "Epoch 120, NOT IMPROVED <total_loss: 0.0430> <bce_loss: 0.0244> <physics_loss: 0.0187> 0.9405->0.9343 (6)    \n",
      "Epoch 121, NOT IMPROVED <total_loss: 0.0433> <bce_loss: 0.0236> <physics_loss: 0.0198> 0.9405->0.9184 (7)    \n",
      "Epoch 122, NOT IMPROVED <total_loss: 0.0419> <bce_loss: 0.0243> <physics_loss: 0.0176> 0.9405->0.9130 (8)    \n",
      "Epoch 123, NOT IMPROVED <total_loss: 0.0405> <bce_loss: 0.0231> <physics_loss: 0.0173> 0.9405->0.9261 (9)    \n",
      "Epoch 124, NOT IMPROVED <total_loss: 0.0404> <bce_loss: 0.0236> <physics_loss: 0.0168> 0.9405->0.9301 (10)    \n",
      "Epoch 125, NOT IMPROVED <total_loss: 0.0452> <bce_loss: 0.0263> <physics_loss: 0.0189> 0.9405->0.9179 (11)    \n",
      "Epoch 126, NOT IMPROVED <total_loss: 0.0427> <bce_loss: 0.0251> <physics_loss: 0.0176> 0.9405->0.9063 (12)    \n",
      "Epoch 127, NOT IMPROVED <total_loss: 0.0420> <bce_loss: 0.0231> <physics_loss: 0.0189> 0.9405->0.9251 (13)    \n",
      "Epoch 128, NOT IMPROVED <total_loss: 0.0408> <bce_loss: 0.0229> <physics_loss: 0.0179> 0.9405->0.9261 (14)    \n",
      "Epoch 129, NOT IMPROVED <total_loss: 0.0385> <bce_loss: 0.0215> <physics_loss: 0.0170> 0.9405->0.9242 (15)    \n",
      "Epoch 130, NOT IMPROVED <total_loss: 0.0387> <bce_loss: 0.0214> <physics_loss: 0.0173> 0.9405->0.9210 (16)    \n",
      "Epoch 131, NOT IMPROVED <total_loss: 0.0408> <bce_loss: 0.0232> <physics_loss: 0.0176> 0.9405->0.9289 (17)    \n",
      "Epoch 132, NOT IMPROVED <total_loss: 0.0404> <bce_loss: 0.0230> <physics_loss: 0.0173> 0.9405->0.9307 (18)    \n",
      "Epoch 133, NOT IMPROVED <total_loss: 0.0403> <bce_loss: 0.0230> <physics_loss: 0.0173> 0.9405->0.9382 (19)    \n",
      "Epoch 134, NOT IMPROVED <total_loss: 0.0429> <bce_loss: 0.0251> <physics_loss: 0.0179> 0.9405->0.9240 (20)    \n",
      "Epoch 135, NOT IMPROVED <total_loss: 0.0438> <bce_loss: 0.0254> <physics_loss: 0.0184> 0.9405->0.9288 (21)    \n",
      "Epoch 136, NOT IMPROVED <total_loss: 0.0425> <bce_loss: 0.0249> <physics_loss: 0.0176> 0.9405->0.9315 (22)    \n",
      "Epoch 137, NOT IMPROVED <total_loss: 0.0429> <bce_loss: 0.0248> <physics_loss: 0.0181> 0.9405->0.9337 (23)    \n",
      "Epoch 138, NOT IMPROVED <total_loss: 0.0413> <bce_loss: 0.0237> <physics_loss: 0.0176> 0.9405->0.9275 (24)    \n",
      "Epoch 139, NOT IMPROVED <total_loss: 0.0413> <bce_loss: 0.0231> <physics_loss: 0.0181> 0.9405->0.9343 (25)    \n",
      "Epoch 140, NOT IMPROVED <total_loss: 0.0423> <bce_loss: 0.0248> <physics_loss: 0.0176> 0.9405->0.9273 (26)    \n",
      "Epoch 141, NOT IMPROVED <total_loss: 0.0446> <bce_loss: 0.0259> <physics_loss: 0.0187> 0.9405->0.9199 (27)    \n",
      "Epoch 142, NOT IMPROVED <total_loss: 0.0424> <bce_loss: 0.0251> <physics_loss: 0.0173> 0.9405->0.9179 (28)    \n",
      "Epoch 143, NOT IMPROVED <total_loss: 0.0412> <bce_loss: 0.0241> <physics_loss: 0.0170> 0.9405->0.9247 (29)    \n",
      "Epoch 144, NOT IMPROVED <total_loss: 0.0430> <bce_loss: 0.0246> <physics_loss: 0.0184> 0.9405->0.9371 (30)    \n",
      "Epoch 145, NOT IMPROVED <total_loss: 0.0437> <bce_loss: 0.0261> <physics_loss: 0.0176> 0.9405->0.9249 (31)    \n",
      "Epoch 146, IMPROVED <total_loss: 0.0427> <bce_loss: 0.0259> <physics_loss: 0.0168> 0.9405->0.9429 (32)     \n",
      "Epoch 147, NOT IMPROVED <total_loss: 0.0432> <bce_loss: 0.0240> <physics_loss: 0.0192> 0.9429->0.9254 (1)    \n",
      "Epoch 148, NOT IMPROVED <total_loss: 0.0459> <bce_loss: 0.0288> <physics_loss: 0.0170> 0.9429->0.9063 (2)    \n",
      "Epoch 149, NOT IMPROVED <total_loss: 0.0466> <bce_loss: 0.0284> <physics_loss: 0.0181> 0.9429->0.9110 (3)    \n",
      "Epoch 150, NOT IMPROVED <total_loss: 0.0445> <bce_loss: 0.0258> <physics_loss: 0.0187> 0.9429->0.9270 (4)    \n",
      "Epoch 151, NOT IMPROVED <total_loss: 0.0409> <bce_loss: 0.0228> <physics_loss: 0.0181> 0.9429->0.9329 (5)    \n",
      "Epoch 152, NOT IMPROVED <total_loss: 0.0422> <bce_loss: 0.0236> <physics_loss: 0.0187> 0.9429->0.9341 (6)    \n",
      "Epoch 153, NOT IMPROVED <total_loss: 0.0429> <bce_loss: 0.0231> <physics_loss: 0.0198> 0.9429->0.9412 (7)    \n",
      "Epoch 154, NOT IMPROVED <total_loss: 0.0423> <bce_loss: 0.0245> <physics_loss: 0.0179> 0.9429->0.9333 (8)    \n",
      "Epoch 155, NOT IMPROVED <total_loss: 0.0459> <bce_loss: 0.0267> <physics_loss: 0.0192> 0.9429->0.9226 (9)    \n",
      "Epoch 156, NOT IMPROVED <total_loss: 0.0451> <bce_loss: 0.0272> <physics_loss: 0.0179> 0.9429->0.9160 (10)    \n",
      "Epoch 157, NOT IMPROVED <total_loss: 0.0409> <bce_loss: 0.0241> <physics_loss: 0.0168> 0.9429->0.9408 (11)    \n",
      "Epoch 158, NOT IMPROVED <total_loss: 0.0405> <bce_loss: 0.0232> <physics_loss: 0.0173> 0.9429->0.9357 (12)    \n",
      "Epoch 159, NOT IMPROVED <total_loss: 0.0453> <bce_loss: 0.0272> <physics_loss: 0.0181> 0.9429->0.9276 (13)    \n",
      "Epoch 160, NOT IMPROVED <total_loss: 0.0440> <bce_loss: 0.0259> <physics_loss: 0.0181> 0.9429->0.9311 (14)    \n",
      "Epoch 161, NOT IMPROVED <total_loss: 0.0411> <bce_loss: 0.0249> <physics_loss: 0.0162> 0.9429->0.9285 (15)    \n",
      "Epoch 162, NOT IMPROVED <total_loss: 0.0428> <bce_loss: 0.0255> <physics_loss: 0.0173> 0.9429->0.9321 (16)    \n",
      "Epoch 163, NOT IMPROVED <total_loss: 0.0434> <bce_loss: 0.0259> <physics_loss: 0.0176> 0.9429->0.9353 (17)    \n",
      "Epoch 164, NOT IMPROVED <total_loss: 0.0466> <bce_loss: 0.0290> <physics_loss: 0.0176> 0.9429->0.9208 (18)    \n",
      "Epoch 165, NOT IMPROVED <total_loss: 0.0453> <bce_loss: 0.0272> <physics_loss: 0.0181> 0.9429->0.9134 (19)    \n",
      "Epoch 166, NOT IMPROVED <total_loss: 0.0413> <bce_loss: 0.0234> <physics_loss: 0.0179> 0.9429->0.9389 (20)    \n",
      "Epoch 167, NOT IMPROVED <total_loss: 0.0406> <bce_loss: 0.0230> <physics_loss: 0.0176> 0.9429->0.9311 (21)    \n",
      "Epoch 168, NOT IMPROVED <total_loss: 0.0428> <bce_loss: 0.0260> <physics_loss: 0.0168> 0.9429->0.9221 (22)    \n",
      "Epoch 169, NOT IMPROVED <total_loss: 0.0459> <bce_loss: 0.0278> <physics_loss: 0.0181> 0.9429->0.9325 (23)    \n",
      "Epoch 170, NOT IMPROVED <total_loss: 0.0430> <bce_loss: 0.0252> <physics_loss: 0.0179> 0.9429->0.9325 (24)    \n",
      "Epoch 171, NOT IMPROVED <total_loss: 0.0409> <bce_loss: 0.0238> <physics_loss: 0.0170> 0.9429->0.9319 (25)    \n",
      "Epoch 172, IMPROVED <total_loss: 0.0401> <bce_loss: 0.0231> <physics_loss: 0.0170> 0.9429->0.9428 (26)     \n",
      "Epoch 173, NOT IMPROVED <total_loss: 0.0462> <bce_loss: 0.0270> <physics_loss: 0.0192> 0.9429->0.9327 (1)    \n",
      "Epoch 174, NOT IMPROVED <total_loss: 0.0460> <bce_loss: 0.0279> <physics_loss: 0.0181> 0.9429->0.9304 (2)    \n",
      "Epoch 175, NOT IMPROVED <total_loss: 0.0428> <bce_loss: 0.0244> <physics_loss: 0.0184> 0.9429->0.9349 (3)    \n",
      "Epoch 176, NOT IMPROVED <total_loss: 0.0436> <bce_loss: 0.0241> <physics_loss: 0.0195> 0.9429->0.9349 (4)    \n",
      "Epoch 177, IMPROVED <total_loss: 0.0413> <bce_loss: 0.0232> <physics_loss: 0.0181> 0.9429->0.9435 (5)     \n",
      "Epoch 178, NOT IMPROVED <total_loss: 0.0455> <bce_loss: 0.0266> <physics_loss: 0.0189> 0.9435->0.9410 (1)    \n",
      "Epoch 179, NOT IMPROVED <total_loss: 0.0429> <bce_loss: 0.0253> <physics_loss: 0.0176> 0.9435->0.9421 (2)    \n",
      "Epoch 180, NOT IMPROVED <total_loss: 0.0435> <bce_loss: 0.0251> <physics_loss: 0.0184> 0.9435->0.9377 (3)    \n",
      "Epoch 181, NOT IMPROVED <total_loss: 0.0423> <bce_loss: 0.0247> <physics_loss: 0.0176> 0.9435->0.9254 (4)    \n",
      "Epoch 182, NOT IMPROVED <total_loss: 0.0433> <bce_loss: 0.0263> <physics_loss: 0.0170> 0.9435->0.9198 (5)    \n",
      "Epoch 183, NOT IMPROVED <total_loss: 0.0442> <bce_loss: 0.0258> <physics_loss: 0.0184> 0.9435->0.9320 (6)    \n",
      "Epoch 184, NOT IMPROVED <total_loss: 0.0469> <bce_loss: 0.0285> <physics_loss: 0.0184> 0.9435->0.9249 (7)    \n",
      "Epoch 185, NOT IMPROVED <total_loss: 0.0449> <bce_loss: 0.0276> <physics_loss: 0.0173> 0.9435->0.9219 (8)    \n",
      "Epoch 186, NOT IMPROVED <total_loss: 0.0413> <bce_loss: 0.0240> <physics_loss: 0.0173> 0.9435->0.9401 (9)    \n",
      "Epoch 187, NOT IMPROVED <total_loss: 0.0420> <bce_loss: 0.0247> <physics_loss: 0.0173> 0.9435->0.9394 (10)    \n",
      "Epoch 188, NOT IMPROVED <total_loss: 0.0409> <bce_loss: 0.0225> <physics_loss: 0.0184> 0.9435->0.9359 (11)    \n",
      "Epoch 189, NOT IMPROVED <total_loss: 0.0408> <bce_loss: 0.0232> <physics_loss: 0.0176> 0.9435->0.9405 (12)    \n",
      "Epoch 190, NOT IMPROVED <total_loss: 0.0450> <bce_loss: 0.0274> <physics_loss: 0.0176> 0.9435->0.9361 (13)    \n",
      "Epoch 191, NOT IMPROVED <total_loss: 0.0422> <bce_loss: 0.0240> <physics_loss: 0.0181> 0.9435->0.9291 (14)    \n",
      "Epoch 192, NOT IMPROVED <total_loss: 0.0410> <bce_loss: 0.0234> <physics_loss: 0.0176> 0.9435->0.9287 (15)    \n",
      "Epoch 193, NOT IMPROVED <total_loss: 0.0414> <bce_loss: 0.0233> <physics_loss: 0.0181> 0.9435->0.9412 (16)    \n",
      "Epoch 194, NOT IMPROVED <total_loss: 0.0442> <bce_loss: 0.0260> <physics_loss: 0.0181> 0.9435->0.9373 (17)    \n",
      "Epoch 195, NOT IMPROVED <total_loss: 0.0417> <bce_loss: 0.0231> <physics_loss: 0.0187> 0.9435->0.9347 (18)    \n",
      "Epoch 196, NOT IMPROVED <total_loss: 0.0433> <bce_loss: 0.0241> <physics_loss: 0.0192> 0.9435->0.9306 (19)    \n",
      "Epoch 197, NOT IMPROVED <total_loss: 0.0443> <bce_loss: 0.0257> <physics_loss: 0.0187> 0.9435->0.9373 (20)    \n",
      "Epoch 198, NOT IMPROVED <total_loss: 0.0449> <bce_loss: 0.0268> <physics_loss: 0.0181> 0.9435->0.9393 (21)    \n",
      "Epoch 199, NOT IMPROVED <total_loss: 0.0429> <bce_loss: 0.0234> <physics_loss: 0.0195> 0.9435->0.9278 (22)    \n",
      "Epoch 200, NOT IMPROVED <total_loss: 0.0423> <bce_loss: 0.0242> <physics_loss: 0.0181> 0.9435->0.9353 (23)    \n",
      "Epoch 201, NOT IMPROVED <total_loss: 0.0437> <bce_loss: 0.0255> <physics_loss: 0.0181> 0.9435->0.9265 (24)    \n",
      "Epoch 202, NOT IMPROVED <total_loss: 0.0437> <bce_loss: 0.0256> <physics_loss: 0.0181> 0.9435->0.9201 (25)    \n",
      "Epoch 203, NOT IMPROVED <total_loss: 0.0427> <bce_loss: 0.0251> <physics_loss: 0.0176> 0.9435->0.9343 (26)    \n",
      "Epoch 204, IMPROVED <total_loss: 0.0418> <bce_loss: 0.0240> <physics_loss: 0.0179> 0.9435->0.9433 (27)     \n",
      "Epoch 205, NOT IMPROVED <total_loss: 0.0427> <bce_loss: 0.0243> <physics_loss: 0.0184> 0.9435->0.9292 (1)    \n",
      "Epoch 206, NOT IMPROVED <total_loss: 0.0432> <bce_loss: 0.0245> <physics_loss: 0.0187> 0.9435->0.9382 (2)    \n",
      "Epoch 207, NOT IMPROVED <total_loss: 0.0436> <bce_loss: 0.0257> <physics_loss: 0.0179> 0.9435->0.9355 (3)    \n",
      "Epoch 208, NOT IMPROVED <total_loss: 0.0436> <bce_loss: 0.0249> <physics_loss: 0.0187> 0.9435->0.9245 (4)    \n",
      "Epoch 209, NOT IMPROVED <total_loss: 0.0424> <bce_loss: 0.0240> <physics_loss: 0.0184> 0.9435->0.9162 (5)    \n",
      "Epoch 210, NOT IMPROVED <total_loss: 0.0445> <bce_loss: 0.0261> <physics_loss: 0.0184> 0.9435->0.9421 (6)    \n",
      "Epoch 211, NOT IMPROVED <total_loss: 0.0441> <bce_loss: 0.0262> <physics_loss: 0.0179> 0.9435->0.9333 (7)    \n",
      "Epoch 212, NOT IMPROVED <total_loss: 0.0452> <bce_loss: 0.0284> <physics_loss: 0.0168> 0.9435->0.9224 (8)    \n",
      "Epoch 213, NOT IMPROVED <total_loss: 0.0457> <bce_loss: 0.0263> <physics_loss: 0.0195> 0.9435->0.9165 (9)    \n",
      "Epoch 214, NOT IMPROVED <total_loss: 0.0431> <bce_loss: 0.0258> <physics_loss: 0.0173> 0.9435->0.9228 (10)    \n",
      "Epoch 215, NOT IMPROVED <total_loss: 0.0426> <bce_loss: 0.0253> <physics_loss: 0.0173> 0.9435->0.9362 (11)    \n",
      "Epoch 216, NOT IMPROVED <total_loss: 0.0443> <bce_loss: 0.0264> <physics_loss: 0.0179> 0.9435->0.9377 (12)    \n",
      "Epoch 217, NOT IMPROVED <total_loss: 0.0464> <bce_loss: 0.0282> <physics_loss: 0.0181> 0.9435->0.9417 (13)    \n",
      "Epoch 218, NOT IMPROVED <total_loss: 0.0423> <bce_loss: 0.0244> <physics_loss: 0.0179> 0.9435->0.9337 (14)    \n",
      "Epoch 219, NOT IMPROVED <total_loss: 0.0421> <bce_loss: 0.0248> <physics_loss: 0.0173> 0.9435->0.9286 (15)    \n",
      "Epoch 220, NOT IMPROVED <total_loss: 0.0460> <bce_loss: 0.0268> <physics_loss: 0.0192> 0.9435->0.9327 (16)    \n",
      "Epoch 221, NOT IMPROVED <total_loss: 0.0445> <bce_loss: 0.0259> <physics_loss: 0.0187> 0.9435->0.9278 (17)    \n",
      "Epoch 222, NOT IMPROVED <total_loss: 0.0410> <bce_loss: 0.0234> <physics_loss: 0.0176> 0.9435->0.9371 (18)    \n",
      "Epoch 223, NOT IMPROVED <total_loss: 0.0474> <bce_loss: 0.0290> <physics_loss: 0.0184> 0.9435->0.9361 (19)    \n",
      "Epoch 224, IMPROVED <total_loss: 0.0437> <bce_loss: 0.0259> <physics_loss: 0.0179> 0.9435->0.9505 (20)     \n",
      "Epoch 225, NOT IMPROVED <total_loss: 0.0451> <bce_loss: 0.0259> <physics_loss: 0.0192> 0.9505->0.9210 (1)    \n",
      "Epoch 226, NOT IMPROVED <total_loss: 0.0441> <bce_loss: 0.0265> <physics_loss: 0.0176> 0.9505->0.9401 (2)    \n",
      "Epoch 227, NOT IMPROVED <total_loss: 0.0418> <bce_loss: 0.0247> <physics_loss: 0.0170> 0.9505->0.9396 (3)    \n",
      "Epoch 228, NOT IMPROVED <total_loss: 0.0450> <bce_loss: 0.0266> <physics_loss: 0.0184> 0.9505->0.9288 (4)    \n",
      "Epoch 229, NOT IMPROVED <total_loss: 0.0464> <bce_loss: 0.0277> <physics_loss: 0.0187> 0.9505->0.9347 (5)    \n",
      "Epoch 230, NOT IMPROVED <total_loss: 0.0453> <bce_loss: 0.0277> <physics_loss: 0.0176> 0.9505->0.9311 (6)    \n",
      "Epoch 231, NOT IMPROVED <total_loss: 0.0441> <bce_loss: 0.0260> <physics_loss: 0.0181> 0.9505->0.9391 (7)    \n",
      "Epoch 232, NOT IMPROVED <total_loss: 0.0445> <bce_loss: 0.0278> <physics_loss: 0.0168> 0.9505->0.9207 (8)    \n",
      "Epoch 233, NOT IMPROVED <total_loss: 0.0402> <bce_loss: 0.0237> <physics_loss: 0.0165> 0.9505->0.9315 (9)    \n",
      "Epoch 234, NOT IMPROVED <total_loss: 0.0466> <bce_loss: 0.0285> <physics_loss: 0.0181> 0.9505->0.9347 (10)    \n",
      "Epoch 235, NOT IMPROVED <total_loss: 0.0440> <bce_loss: 0.0261> <physics_loss: 0.0179> 0.9505->0.9331 (11)    \n",
      "Epoch 236, NOT IMPROVED <total_loss: 0.0421> <bce_loss: 0.0245> <physics_loss: 0.0176> 0.9505->0.9398 (12)    \n",
      "Epoch 237, NOT IMPROVED <total_loss: 0.0413> <bce_loss: 0.0248> <physics_loss: 0.0165> 0.9505->0.9378 (13)    \n",
      "Epoch 238, NOT IMPROVED <total_loss: 0.0429> <bce_loss: 0.0259> <physics_loss: 0.0170> 0.9505->0.9396 (14)    \n",
      "Epoch 239, NOT IMPROVED <total_loss: 0.0493> <bce_loss: 0.0298> <physics_loss: 0.0195> 0.9505->0.9304 (15)    \n",
      "Epoch 240, NOT IMPROVED <total_loss: 0.0444> <bce_loss: 0.0265> <physics_loss: 0.0179> 0.9505->0.9329 (16)    \n",
      "Epoch 241, NOT IMPROVED <total_loss: 0.0422> <bce_loss: 0.0246> <physics_loss: 0.0176> 0.9505->0.9331 (17)    \n",
      "Epoch 242, NOT IMPROVED <total_loss: 0.0433> <bce_loss: 0.0254> <physics_loss: 0.0179> 0.9505->0.9345 (18)    \n",
      "Epoch 243, NOT IMPROVED <total_loss: 0.0458> <bce_loss: 0.0271> <physics_loss: 0.0187> 0.9505->0.9244 (19)    \n",
      "Epoch 244, NOT IMPROVED <total_loss: 0.0473> <bce_loss: 0.0286> <physics_loss: 0.0187> 0.9505->0.9318 (20)    \n",
      "Epoch 245, NOT IMPROVED <total_loss: 0.0448> <bce_loss: 0.0267> <physics_loss: 0.0181> 0.9505->0.9371 (21)    \n",
      "Epoch 246, NOT IMPROVED <total_loss: 0.0459> <bce_loss: 0.0275> <physics_loss: 0.0184> 0.9505->0.9325 (22)    \n",
      "Epoch 247, NOT IMPROVED <total_loss: 0.0414> <bce_loss: 0.0239> <physics_loss: 0.0176> 0.9505->0.9291 (23)    \n",
      "Epoch 248, NOT IMPROVED <total_loss: 0.0457> <bce_loss: 0.0278> <physics_loss: 0.0179> 0.9505->0.9401 (24)    \n",
      "Epoch 249, NOT IMPROVED <total_loss: 0.0443> <bce_loss: 0.0259> <physics_loss: 0.0184> 0.9505->0.9361 (25)    \n",
      "Epoch 250, NOT IMPROVED <total_loss: 0.0446> <bce_loss: 0.0259> <physics_loss: 0.0187> 0.9505->0.9258 (26)    \n",
      "Epoch 251, NOT IMPROVED <total_loss: 0.0451> <bce_loss: 0.0264> <physics_loss: 0.0187> 0.9505->0.9269 (27)    \n",
      "Epoch 252, NOT IMPROVED <total_loss: 0.0443> <bce_loss: 0.0251> <physics_loss: 0.0192> 0.9505->0.9158 (28)    \n",
      "Epoch 253, NOT IMPROVED <total_loss: 0.0483> <bce_loss: 0.0285> <physics_loss: 0.0198> 0.9505->0.9195 (29)    \n",
      "Epoch 254, NOT IMPROVED <total_loss: 0.0509> <bce_loss: 0.0314> <physics_loss: 0.0195> 0.9505->0.9260 (30)    \n",
      "Epoch 255, NOT IMPROVED <total_loss: 0.0451> <bce_loss: 0.0264> <physics_loss: 0.0187> 0.9505->0.9369 (31)    \n",
      "Epoch 256, NOT IMPROVED <total_loss: 0.0478> <bce_loss: 0.0289> <physics_loss: 0.0189> 0.9505->0.9323 (32)    \n",
      "Epoch 257, NOT IMPROVED <total_loss: 0.0501> <bce_loss: 0.0309> <physics_loss: 0.0192> 0.9505->0.9268 (33)    \n",
      "Epoch 258, NOT IMPROVED <total_loss: 0.0468> <bce_loss: 0.0278> <physics_loss: 0.0189> 0.9505->0.9327 (34)    \n",
      "Epoch 259, NOT IMPROVED <total_loss: 0.0459> <bce_loss: 0.0275> <physics_loss: 0.0184> 0.9505->0.9280 (35)    \n",
      "Epoch 260, NOT IMPROVED <total_loss: 0.0446> <bce_loss: 0.0273> <physics_loss: 0.0173> 0.9505->0.9351 (36)    \n",
      "Epoch 261, NOT IMPROVED <total_loss: 0.0434> <bce_loss: 0.0260> <physics_loss: 0.0173> 0.9505->0.9329 (37)    \n",
      "Epoch 262, NOT IMPROVED <total_loss: 0.0448> <bce_loss: 0.0267> <physics_loss: 0.0181> 0.9505->0.9389 (38)    \n",
      "Epoch 263, NOT IMPROVED <total_loss: 0.0500> <bce_loss: 0.0321> <physics_loss: 0.0179> 0.9505->0.9270 (39)    \n",
      "Epoch 264, NOT IMPROVED <total_loss: 0.0492> <bce_loss: 0.0310> <physics_loss: 0.0181> 0.9505->0.9355 (40)    \n",
      "Epoch 265, NOT IMPROVED <total_loss: 0.0448> <bce_loss: 0.0275> <physics_loss: 0.0173> 0.9505->0.9319 (41)    \n",
      "Epoch 266, NOT IMPROVED <total_loss: 0.0439> <bce_loss: 0.0258> <physics_loss: 0.0181> 0.9505->0.9353 (42)    \n",
      "Epoch 267, NOT IMPROVED <total_loss: 0.0450> <bce_loss: 0.0268> <physics_loss: 0.0181> 0.9505->0.9325 (43)    \n",
      "Epoch 268, NOT IMPROVED <total_loss: 0.0487> <bce_loss: 0.0316> <physics_loss: 0.0170> 0.9505->0.9160 (44)    \n",
      "Epoch 269, NOT IMPROVED <total_loss: 0.0438> <bce_loss: 0.0254> <physics_loss: 0.0184> 0.9505->0.9277 (45)    \n",
      "Epoch 270, NOT IMPROVED <total_loss: 0.0422> <bce_loss: 0.0241> <physics_loss: 0.0181> 0.9505->0.9431 (46)    \n",
      "Epoch 271, NOT IMPROVED <total_loss: 0.0446> <bce_loss: 0.0256> <physics_loss: 0.0189> 0.9505->0.9335 (47)    \n",
      "Epoch 272, NOT IMPROVED <total_loss: 0.0423> <bce_loss: 0.0233> <physics_loss: 0.0189> 0.9505->0.9377 (48)    \n",
      "Epoch 273, NOT IMPROVED <total_loss: 0.0444> <bce_loss: 0.0257> <physics_loss: 0.0187> 0.9505->0.9419 (49)    \n",
      "Epoch 274, NOT IMPROVED <total_loss: 0.0442> <bce_loss: 0.0261> <physics_loss: 0.0181> 0.9505->0.9472 (50)    \n",
      "Epoch 275, NOT IMPROVED <total_loss: 0.0439> <bce_loss: 0.0255> <physics_loss: 0.0184> 0.9505->0.9377 (51)    \n",
      "Epoch 276, NOT IMPROVED <total_loss: 0.0469> <bce_loss: 0.0282> <physics_loss: 0.0187> 0.9505->0.9309 (52)    \n",
      "Epoch 277, NOT IMPROVED <total_loss: 0.0434> <bce_loss: 0.0250> <physics_loss: 0.0184> 0.9505->0.9375 (53)    \n",
      "Epoch 278, NOT IMPROVED <total_loss: 0.0461> <bce_loss: 0.0280> <physics_loss: 0.0181> 0.9505->0.9375 (54)    \n",
      "Epoch 279, NOT IMPROVED <total_loss: 0.0462> <bce_loss: 0.0270> <physics_loss: 0.0192> 0.9505->0.9370 (55)    \n",
      "Epoch 280, NOT IMPROVED <total_loss: 0.0446> <bce_loss: 0.0263> <physics_loss: 0.0184> 0.9505->0.9445 (56)    \n",
      "Epoch 281, NOT IMPROVED <total_loss: 0.0428> <bce_loss: 0.0250> <physics_loss: 0.0179> 0.9505->0.9353 (57)    \n",
      "Epoch 282, NOT IMPROVED <total_loss: 0.0419> <bce_loss: 0.0251> <physics_loss: 0.0168> 0.9505->0.9392 (58)    \n",
      "Epoch 283, NOT IMPROVED <total_loss: 0.0443> <bce_loss: 0.0265> <physics_loss: 0.0179> 0.9505->0.9303 (59)    \n",
      "Epoch 284, NOT IMPROVED <total_loss: 0.0475> <bce_loss: 0.0294> <physics_loss: 0.0181> 0.9505->0.9251 (60)    \n",
      "Epoch 285, NOT IMPROVED <total_loss: 0.0482> <bce_loss: 0.0296> <physics_loss: 0.0187> 0.9505->0.9324 (61)    \n",
      "Epoch 286, NOT IMPROVED <total_loss: 0.0518> <bce_loss: 0.0317> <physics_loss: 0.0200> 0.9505->0.9244 (62)    \n",
      "Epoch 287, NOT IMPROVED <total_loss: 0.0472> <bce_loss: 0.0286> <physics_loss: 0.0187> 0.9505->0.9290 (63)    \n",
      "Epoch 288, NOT IMPROVED <total_loss: 0.0497> <bce_loss: 0.0310> <physics_loss: 0.0187> 0.9505->0.9329 (64)    \n",
      "Epoch 289, NOT IMPROVED <total_loss: 0.0455> <bce_loss: 0.0276> <physics_loss: 0.0179> 0.9505->0.9355 (65)    \n",
      "Epoch 290, NOT IMPROVED <total_loss: 0.0450> <bce_loss: 0.0269> <physics_loss: 0.0181> 0.9505->0.9363 (66)    \n",
      "Epoch 291, NOT IMPROVED <total_loss: 0.0482> <bce_loss: 0.0306> <physics_loss: 0.0176> 0.9505->0.9327 (67)    \n",
      "Epoch 292, NOT IMPROVED <total_loss: 0.0479> <bce_loss: 0.0292> <physics_loss: 0.0187> 0.9505->0.9272 (68)    \n",
      "Epoch 293, NOT IMPROVED <total_loss: 0.0422> <bce_loss: 0.0249> <physics_loss: 0.0173> 0.9505->0.9364 (69)    \n",
      "Epoch 294, NOT IMPROVED <total_loss: 0.0449> <bce_loss: 0.0287> <physics_loss: 0.0162> 0.9505->0.9294 (70)    \n",
      "Epoch 295, NOT IMPROVED <total_loss: 0.0437> <bce_loss: 0.0256> <physics_loss: 0.0181> 0.9505->0.9302 (71)    \n",
      "Epoch 296, NOT IMPROVED <total_loss: 0.0439> <bce_loss: 0.0255> <physics_loss: 0.0184> 0.9505->0.9318 (72)    \n",
      "Epoch 297, NOT IMPROVED <total_loss: 0.0584> <bce_loss: 0.0400> <physics_loss: 0.0184> 0.9505->0.9296 (73)    \n",
      "Epoch 298, NOT IMPROVED <total_loss: 0.0494> <bce_loss: 0.0310> <physics_loss: 0.0184> 0.9505->0.9212 (74)    \n",
      "Epoch 299, NOT IMPROVED <total_loss: 0.0454> <bce_loss: 0.0270> <physics_loss: 0.0184> 0.9505->0.9401 (75)    \n",
      "Epoch 300, NOT IMPROVED <total_loss: 0.0445> <bce_loss: 0.0255> <physics_loss: 0.0189> 0.9505->0.9290 (76)    \n",
      "Epoch 301, NOT IMPROVED <total_loss: 0.0446> <bce_loss: 0.0262> <physics_loss: 0.0184> 0.9505->0.9295 (77)    \n",
      "Epoch 302, NOT IMPROVED <total_loss: 0.0409> <bce_loss: 0.0231> <physics_loss: 0.0179> 0.9505->0.9309 (78)    \n",
      "Epoch 303, NOT IMPROVED <total_loss: 0.0417> <bce_loss: 0.0238> <physics_loss: 0.0179> 0.9505->0.9307 (79)    \n",
      "Epoch 304, NOT IMPROVED <total_loss: 0.0455> <bce_loss: 0.0274> <physics_loss: 0.0181> 0.9505->0.9168 (80)    \n",
      "Epoch 305, NOT IMPROVED <total_loss: 0.0481> <bce_loss: 0.0289> <physics_loss: 0.0192> 0.9505->0.9304 (81)    \n",
      "Epoch 306, NOT IMPROVED <total_loss: 0.0440> <bce_loss: 0.0256> <physics_loss: 0.0184> 0.9505->0.9341 (82)    \n",
      "Epoch 307, NOT IMPROVED <total_loss: 0.0453> <bce_loss: 0.0271> <physics_loss: 0.0181> 0.9505->0.9290 (83)    \n",
      "Epoch 308, NOT IMPROVED <total_loss: 0.0460> <bce_loss: 0.0300> <physics_loss: 0.0160> 0.9505->0.9127 (84)    \n",
      "Epoch 309, NOT IMPROVED <total_loss: 0.0466> <bce_loss: 0.0276> <physics_loss: 0.0189> 0.9505->0.9355 (85)    \n",
      "Epoch 310, NOT IMPROVED <total_loss: 0.0424> <bce_loss: 0.0251> <physics_loss: 0.0173> 0.9505->0.9321 (86)    \n",
      "Epoch 311, NOT IMPROVED <total_loss: 0.0417> <bce_loss: 0.0233> <physics_loss: 0.0184> 0.9505->0.9399 (87)    \n",
      "Epoch 312, NOT IMPROVED <total_loss: 0.0420> <bce_loss: 0.0247> <physics_loss: 0.0173> 0.9505->0.9335 (88)    \n",
      "Epoch 313, NOT IMPROVED <total_loss: 0.0460> <bce_loss: 0.0276> <physics_loss: 0.0184> 0.9505->0.9357 (89)    \n",
      "Epoch 314, NOT IMPROVED <total_loss: 0.0426> <bce_loss: 0.0244> <physics_loss: 0.0181> 0.9505->0.9371 (90)    \n",
      "Epoch 315, NOT IMPROVED <total_loss: 0.0446> <bce_loss: 0.0264> <physics_loss: 0.0181> 0.9505->0.9325 (91)    \n",
      "Epoch 316, NOT IMPROVED <total_loss: 0.0449> <bce_loss: 0.0268> <physics_loss: 0.0181> 0.9505->0.9238 (92)    \n",
      "Epoch 317, NOT IMPROVED <total_loss: 0.0418> <bce_loss: 0.0242> <physics_loss: 0.0176> 0.9505->0.9285 (93)    \n",
      "Epoch 318, NOT IMPROVED <total_loss: 0.0434> <bce_loss: 0.0256> <physics_loss: 0.0179> 0.9505->0.9302 (94)    \n",
      "Epoch 319, NOT IMPROVED <total_loss: 0.0453> <bce_loss: 0.0271> <physics_loss: 0.0181> 0.9505->0.9224 (95)    \n",
      "Epoch 320, NOT IMPROVED <total_loss: 0.0420> <bce_loss: 0.0231> <physics_loss: 0.0189> 0.9505->0.9367 (96)    \n",
      "Epoch 321, NOT IMPROVED <total_loss: 0.0418> <bce_loss: 0.0234> <physics_loss: 0.0184> 0.9505->0.9297 (97)    \n",
      "Epoch 322, NOT IMPROVED <total_loss: 0.0420> <bce_loss: 0.0236> <physics_loss: 0.0184> 0.9505->0.9343 (98)    \n",
      "Epoch 323, NOT IMPROVED <total_loss: 0.0423> <bce_loss: 0.0247> <physics_loss: 0.0176> 0.9505->0.9270 (99)    \n",
      "Epoch 324, NOT IMPROVED <total_loss: 0.0438> <bce_loss: 0.0254> <physics_loss: 0.0184> 0.9505->0.9361 (100)    \n",
      "Epoch 325, NOT IMPROVED <total_loss: 0.0458> <bce_loss: 0.0271> <physics_loss: 0.0187> 0.9505->0.9379 (101)    \n",
      "Epoch 326, NOT IMPROVED <total_loss: 0.0456> <bce_loss: 0.0272> <physics_loss: 0.0184> 0.9505->0.9315 (102)    \n",
      "Epoch 327, NOT IMPROVED <total_loss: 0.0468> <bce_loss: 0.0284> <physics_loss: 0.0184> 0.9505->0.9327 (103)    \n",
      "Epoch 328, NOT IMPROVED <total_loss: 0.0486> <bce_loss: 0.0296> <physics_loss: 0.0189> 0.9505->0.9251 (104)    \n",
      "Epoch 329, NOT IMPROVED <total_loss: 0.0449> <bce_loss: 0.0262> <physics_loss: 0.0187> 0.9505->0.9249 (105)    \n",
      "Epoch 330, NOT IMPROVED <total_loss: 0.0424> <bce_loss: 0.0251> <physics_loss: 0.0173> 0.9505->0.9351 (106)    \n",
      "Epoch 331, NOT IMPROVED <total_loss: 0.0469> <bce_loss: 0.0296> <physics_loss: 0.0173> 0.9505->0.9371 (107)    \n",
      "Epoch 332, NOT IMPROVED <total_loss: 0.0474> <bce_loss: 0.0290> <physics_loss: 0.0184> 0.9505->0.9403 (108)    \n",
      "Epoch 333, NOT IMPROVED <total_loss: 0.0447> <bce_loss: 0.0258> <physics_loss: 0.0189> 0.9505->0.9325 (109)    \n",
      "Epoch 334, NOT IMPROVED <total_loss: 0.0452> <bce_loss: 0.0284> <physics_loss: 0.0168> 0.9505->0.9252 (110)    \n",
      "Epoch 335, NOT IMPROVED <total_loss: 0.0442> <bce_loss: 0.0271> <physics_loss: 0.0170> 0.9505->0.9385 (111)    \n",
      "Epoch 336, IMPROVED <total_loss: 0.0429> <bce_loss: 0.0245> <physics_loss: 0.0184> 0.9505->0.9524 (112)     \n",
      "Epoch 337, NOT IMPROVED <total_loss: 0.0454> <bce_loss: 0.0272> <physics_loss: 0.0181> 0.9524->0.9417 (1)    \n",
      "Epoch 338, NOT IMPROVED <total_loss: 0.0446> <bce_loss: 0.0262> <physics_loss: 0.0184> 0.9524->0.9407 (2)    \n",
      "Epoch 339, NOT IMPROVED <total_loss: 0.0482> <bce_loss: 0.0306> <physics_loss: 0.0176> 0.9524->0.9313 (3)    \n",
      "Epoch 340, NOT IMPROVED <total_loss: 0.0456> <bce_loss: 0.0275> <physics_loss: 0.0181> 0.9524->0.9327 (4)    \n",
      "Epoch 341, NOT IMPROVED <total_loss: 0.0467> <bce_loss: 0.0288> <physics_loss: 0.0179> 0.9524->0.9254 (5)    \n",
      "Epoch 342, NOT IMPROVED <total_loss: 0.0432> <bce_loss: 0.0251> <physics_loss: 0.0181> 0.9524->0.9373 (6)    \n",
      "Epoch 343, NOT IMPROVED <total_loss: 0.0439> <bce_loss: 0.0253> <physics_loss: 0.0187> 0.9524->0.9371 (7)    \n",
      "Epoch 344, NOT IMPROVED <total_loss: 0.0426> <bce_loss: 0.0242> <physics_loss: 0.0184> 0.9524->0.9415 (8)    \n",
      "Epoch 345, NOT IMPROVED <total_loss: 0.0460> <bce_loss: 0.0276> <physics_loss: 0.0184> 0.9524->0.9393 (9)    \n",
      "Epoch 346, NOT IMPROVED <total_loss: 0.0433> <bce_loss: 0.0252> <physics_loss: 0.0181> 0.9524->0.9407 (10)    \n",
      "Epoch 347, NOT IMPROVED <total_loss: 0.0440> <bce_loss: 0.0259> <physics_loss: 0.0181> 0.9524->0.9302 (11)    \n",
      "Epoch 348, NOT IMPROVED <total_loss: 0.0438> <bce_loss: 0.0248> <physics_loss: 0.0189> 0.9524->0.9322 (12)    \n",
      "Epoch 349, NOT IMPROVED <total_loss: 0.0421> <bce_loss: 0.0247> <physics_loss: 0.0173> 0.9524->0.9383 (13)    \n",
      "Epoch 350, NOT IMPROVED <total_loss: 0.0481> <bce_loss: 0.0297> <physics_loss: 0.0184> 0.9524->0.9331 (14)    \n",
      "Epoch 351, NOT IMPROVED <total_loss: 0.0432> <bce_loss: 0.0254> <physics_loss: 0.0179> 0.9524->0.9339 (15)    \n",
      "Epoch 352, NOT IMPROVED <total_loss: 0.0464> <bce_loss: 0.0283> <physics_loss: 0.0181> 0.9524->0.9222 (16)    \n",
      "Epoch 353, NOT IMPROVED <total_loss: 0.0455> <bce_loss: 0.0265> <physics_loss: 0.0189> 0.9524->0.9235 (17)    \n",
      "Epoch 354, NOT IMPROVED <total_loss: 0.0455> <bce_loss: 0.0263> <physics_loss: 0.0192> 0.9524->0.9322 (18)    \n",
      "Epoch 355, NOT IMPROVED <total_loss: 0.0456> <bce_loss: 0.0267> <physics_loss: 0.0189> 0.9524->0.9329 (19)    \n",
      "Epoch 356, NOT IMPROVED <total_loss: 0.0493> <bce_loss: 0.0306> <physics_loss: 0.0187> 0.9524->0.9412 (20)    \n",
      "Epoch 357, NOT IMPROVED <total_loss: 0.0486> <bce_loss: 0.0302> <physics_loss: 0.0184> 0.9524->0.9343 (21)    \n",
      "Epoch 358, NOT IMPROVED <total_loss: 0.0466> <bce_loss: 0.0274> <physics_loss: 0.0192> 0.9524->0.9337 (22)    \n",
      "Epoch 359, NOT IMPROVED <total_loss: 0.0437> <bce_loss: 0.0264> <physics_loss: 0.0173> 0.9524->0.9459 (23)    \n",
      "Epoch 360, NOT IMPROVED <total_loss: 0.0458> <bce_loss: 0.0277> <physics_loss: 0.0181> 0.9524->0.9347 (24)    \n",
      "Epoch 361, NOT IMPROVED <total_loss: 0.0467> <bce_loss: 0.0270> <physics_loss: 0.0198> 0.9524->0.9357 (25)    \n",
      "Epoch 362, NOT IMPROVED <total_loss: 0.0463> <bce_loss: 0.0271> <physics_loss: 0.0192> 0.9524->0.9296 (26)    \n",
      "Epoch 363, NOT IMPROVED <total_loss: 0.0454> <bce_loss: 0.0267> <physics_loss: 0.0187> 0.9524->0.9318 (27)    \n",
      "Epoch 364, NOT IMPROVED <total_loss: 0.0445> <bce_loss: 0.0261> <physics_loss: 0.0184> 0.9524->0.9414 (28)    \n",
      "Epoch 365, NOT IMPROVED <total_loss: 0.0452> <bce_loss: 0.0271> <physics_loss: 0.0181> 0.9524->0.9419 (29)    \n",
      "Epoch 366, NOT IMPROVED <total_loss: 0.0448> <bce_loss: 0.0275> <physics_loss: 0.0173> 0.9524->0.9287 (30)    \n",
      "Epoch 367, NOT IMPROVED <total_loss: 0.0448> <bce_loss: 0.0264> <physics_loss: 0.0184> 0.9524->0.9401 (31)    \n",
      "Epoch 368, NOT IMPROVED <total_loss: 0.0409> <bce_loss: 0.0230> <physics_loss: 0.0179> 0.9524->0.9297 (32)    \n",
      "Epoch 369, NOT IMPROVED <total_loss: 0.0447> <bce_loss: 0.0263> <physics_loss: 0.0184> 0.9524->0.9407 (33)    \n",
      "Epoch 370, NOT IMPROVED <total_loss: 0.0442> <bce_loss: 0.0261> <physics_loss: 0.0181> 0.9524->0.9313 (34)    \n",
      "Epoch 371, NOT IMPROVED <total_loss: 0.0442> <bce_loss: 0.0263> <physics_loss: 0.0179> 0.9524->0.9436 (35)    \n",
      "Epoch 372, NOT IMPROVED <total_loss: 0.0410> <bce_loss: 0.0228> <physics_loss: 0.0181> 0.9524->0.9333 (36)    \n",
      "Epoch 373, NOT IMPROVED <total_loss: 0.0456> <bce_loss: 0.0269> <physics_loss: 0.0187> 0.9524->0.9249 (37)    \n",
      "Epoch 374, NOT IMPROVED <total_loss: 0.0455> <bce_loss: 0.0271> <physics_loss: 0.0184> 0.9524->0.9247 (38)    \n",
      "Epoch 375, NOT IMPROVED <total_loss: 0.0410> <bce_loss: 0.0229> <physics_loss: 0.0181> 0.9524->0.9327 (39)    \n",
      "Epoch 376, NOT IMPROVED <total_loss: 0.0440> <bce_loss: 0.0264> <physics_loss: 0.0176> 0.9524->0.9323 (40)    \n",
      "Epoch 377, NOT IMPROVED <total_loss: 0.0447> <bce_loss: 0.0260> <physics_loss: 0.0187> 0.9524->0.9363 (41)    \n",
      "Epoch 378, NOT IMPROVED <total_loss: 0.0461> <bce_loss: 0.0280> <physics_loss: 0.0181> 0.9524->0.9391 (42)    \n",
      "Epoch 379, NOT IMPROVED <total_loss: 0.0472> <bce_loss: 0.0294> <physics_loss: 0.0179> 0.9524->0.9279 (43)    \n",
      "Epoch 380, NOT IMPROVED <total_loss: 0.0433> <bce_loss: 0.0254> <physics_loss: 0.0179> 0.9524->0.9371 (44)    \n",
      "Epoch 381, NOT IMPROVED <total_loss: 0.0441> <bce_loss: 0.0251> <physics_loss: 0.0189> 0.9524->0.9379 (45)    \n",
      "Epoch 382, NOT IMPROVED <total_loss: 0.0441> <bce_loss: 0.0262> <physics_loss: 0.0179> 0.9524->0.9383 (46)    \n",
      "Epoch 383, NOT IMPROVED <total_loss: 0.0444> <bce_loss: 0.0263> <physics_loss: 0.0181> 0.9524->0.9341 (47)    \n",
      "Epoch 384, NOT IMPROVED <total_loss: 0.0460> <bce_loss: 0.0271> <physics_loss: 0.0189> 0.9524->0.9351 (48)    \n",
      "Epoch 385, NOT IMPROVED <total_loss: 0.0439> <bce_loss: 0.0249> <physics_loss: 0.0189> 0.9524->0.9320 (49)    \n",
      "Epoch 386, NOT IMPROVED <total_loss: 0.0434> <bce_loss: 0.0256> <physics_loss: 0.0179> 0.9524->0.9302 (50)    \n",
      "Epoch 387, NOT IMPROVED <total_loss: 0.0475> <bce_loss: 0.0286> <physics_loss: 0.0189> 0.9524->0.9320 (51)    \n",
      "Epoch 388, NOT IMPROVED <total_loss: 0.0481> <bce_loss: 0.0292> <physics_loss: 0.0189> 0.9524->0.9231 (52)    \n",
      "Epoch 389, NOT IMPROVED <total_loss: 0.0453> <bce_loss: 0.0271> <physics_loss: 0.0181> 0.9524->0.9357 (53)    \n",
      "Epoch 390, NOT IMPROVED <total_loss: 0.0460> <bce_loss: 0.0273> <physics_loss: 0.0187> 0.9524->0.9320 (54)    \n",
      "Epoch 391, NOT IMPROVED <total_loss: 0.0481> <bce_loss: 0.0294> <physics_loss: 0.0187> 0.9524->0.9306 (55)    \n",
      "Epoch 392, NOT IMPROVED <total_loss: 0.0486> <bce_loss: 0.0304> <physics_loss: 0.0181> 0.9524->0.9297 (56)    \n",
      "Epoch 393, NOT IMPROVED <total_loss: 0.0469> <bce_loss: 0.0288> <physics_loss: 0.0181> 0.9524->0.9377 (57)    \n",
      "Epoch 394, NOT IMPROVED <total_loss: 0.0465> <bce_loss: 0.0295> <physics_loss: 0.0170> 0.9524->0.9263 (58)    \n",
      "Epoch 395, NOT IMPROVED <total_loss: 0.0467> <bce_loss: 0.0297> <physics_loss: 0.0170> 0.9524->0.9287 (59)    \n",
      "Epoch 396, NOT IMPROVED <total_loss: 0.0452> <bce_loss: 0.0271> <physics_loss: 0.0181> 0.9524->0.9417 (60)    \n",
      "Epoch 397, NOT IMPROVED <total_loss: 0.0442> <bce_loss: 0.0258> <physics_loss: 0.0184> 0.9524->0.9256 (61)    \n",
      "Epoch 398, NOT IMPROVED <total_loss: 0.0439> <bce_loss: 0.0261> <physics_loss: 0.0179> 0.9524->0.9353 (62)    \n",
      "Epoch 399, NOT IMPROVED <total_loss: 0.0454> <bce_loss: 0.0284> <physics_loss: 0.0170> 0.9524->0.9333 (63)    \n",
      "Epoch 400, NOT IMPROVED <total_loss: 0.0454> <bce_loss: 0.0281> <physics_loss: 0.0173> 0.9524->0.9293 (64)    \n",
      "Epoch 401, NOT IMPROVED <total_loss: 0.0464> <bce_loss: 0.0290> <physics_loss: 0.0173> 0.9524->0.9329 (65)    \n",
      "Epoch 402, NOT IMPROVED <total_loss: 0.0476> <bce_loss: 0.0300> <physics_loss: 0.0176> 0.9524->0.9339 (66)    \n",
      "Epoch 403, NOT IMPROVED <total_loss: 0.0432> <bce_loss: 0.0253> <physics_loss: 0.0179> 0.9524->0.9417 (67)    \n",
      "Epoch 404, NOT IMPROVED <total_loss: 0.0468> <bce_loss: 0.0279> <physics_loss: 0.0189> 0.9524->0.9300 (68)    \n",
      "Epoch 405, NOT IMPROVED <total_loss: 0.0497> <bce_loss: 0.0319> <physics_loss: 0.0179> 0.9524->0.9337 (69)    \n",
      "Epoch 406, NOT IMPROVED <total_loss: 0.0458> <bce_loss: 0.0285> <physics_loss: 0.0173> 0.9524->0.9349 (70)    \n",
      "Epoch 407, NOT IMPROVED <total_loss: 0.0453> <bce_loss: 0.0269> <physics_loss: 0.0184> 0.9524->0.9349 (71)    \n",
      "Epoch 408, NOT IMPROVED <total_loss: 0.0416> <bce_loss: 0.0237> <physics_loss: 0.0179> 0.9524->0.9401 (72)    \n",
      "Epoch 409, NOT IMPROVED <total_loss: 0.0453> <bce_loss: 0.0277> <physics_loss: 0.0176> 0.9524->0.9447 (73)    \n",
      "Epoch 410, NOT IMPROVED <total_loss: 0.0472> <bce_loss: 0.0293> <physics_loss: 0.0179> 0.9524->0.9347 (74)    \n",
      "Epoch 411, NOT IMPROVED <total_loss: 0.0433> <bce_loss: 0.0254> <physics_loss: 0.0179> 0.9524->0.9325 (75)    \n",
      "Epoch 412, NOT IMPROVED <total_loss: 0.0442> <bce_loss: 0.0256> <physics_loss: 0.0187> 0.9524->0.9302 (76)    \n",
      "Epoch 413, NOT IMPROVED <total_loss: 0.0429> <bce_loss: 0.0247> <physics_loss: 0.0181> 0.9524->0.9329 (77)    \n",
      "Epoch 414, NOT IMPROVED <total_loss: 0.0472> <bce_loss: 0.0290> <physics_loss: 0.0181> 0.9524->0.9375 (78)    \n",
      "Epoch 415, NOT IMPROVED <total_loss: 0.0498> <bce_loss: 0.0316> <physics_loss: 0.0181> 0.9524->0.9256 (79)    \n",
      "Epoch 416, NOT IMPROVED <total_loss: 0.0483> <bce_loss: 0.0291> <physics_loss: 0.0192> 0.9524->0.9393 (80)    \n",
      "Epoch 417, NOT IMPROVED <total_loss: 0.0498> <bce_loss: 0.0319> <physics_loss: 0.0179> 0.9524->0.9311 (81)    \n",
      "Epoch 418, NOT IMPROVED <total_loss: 0.0469> <bce_loss: 0.0288> <physics_loss: 0.0181> 0.9524->0.9421 (82)    \n",
      "Epoch 419, NOT IMPROVED <total_loss: 0.0473> <bce_loss: 0.0289> <physics_loss: 0.0184> 0.9524->0.9286 (83)    \n",
      "Epoch 420, NOT IMPROVED <total_loss: 0.0486> <bce_loss: 0.0297> <physics_loss: 0.0189> 0.9524->0.9337 (84)    \n",
      "Epoch 421, NOT IMPROVED <total_loss: 0.0467> <bce_loss: 0.0286> <physics_loss: 0.0181> 0.9524->0.9431 (85)    \n",
      "Epoch 422, NOT IMPROVED <total_loss: 0.0441> <bce_loss: 0.0255> <physics_loss: 0.0187> 0.9524->0.9422 (86)    \n",
      "Epoch 423, NOT IMPROVED <total_loss: 0.0455> <bce_loss: 0.0276> <physics_loss: 0.0179> 0.9524->0.9311 (87)    \n",
      "Epoch 424, NOT IMPROVED <total_loss: 0.0454> <bce_loss: 0.0278> <physics_loss: 0.0176> 0.9524->0.9367 (88)    \n",
      "Epoch 425, NOT IMPROVED <total_loss: 0.0442> <bce_loss: 0.0264> <physics_loss: 0.0179> 0.9524->0.9387 (89)    \n",
      "Epoch 426, NOT IMPROVED <total_loss: 0.0459> <bce_loss: 0.0280> <physics_loss: 0.0179> 0.9524->0.9341 (90)    \n",
      "Epoch 427, NOT IMPROVED <total_loss: 0.0436> <bce_loss: 0.0255> <physics_loss: 0.0181> 0.9524->0.9383 (91)    \n",
      "Epoch 428, NOT IMPROVED <total_loss: 0.0423> <bce_loss: 0.0242> <physics_loss: 0.0181> 0.9524->0.9414 (92)    \n",
      "Epoch 429, NOT IMPROVED <total_loss: 0.0450> <bce_loss: 0.0272> <physics_loss: 0.0179> 0.9524->0.9401 (93)    \n",
      "Epoch 430, NOT IMPROVED <total_loss: 0.0432> <bce_loss: 0.0251> <physics_loss: 0.0181> 0.9524->0.9429 (94)    \n",
      "Epoch 431, NOT IMPROVED <total_loss: 0.0452> <bce_loss: 0.0271> <physics_loss: 0.0181> 0.9524->0.9387 (95)    \n",
      "Epoch 432, NOT IMPROVED <total_loss: 0.0441> <bce_loss: 0.0260> <physics_loss: 0.0181> 0.9524->0.9435 (96)    \n",
      "Epoch 433, NOT IMPROVED <total_loss: 0.0453> <bce_loss: 0.0274> <physics_loss: 0.0179> 0.9524->0.9355 (97)    \n",
      "Epoch 434, NOT IMPROVED <total_loss: 0.0483> <bce_loss: 0.0307> <physics_loss: 0.0176> 0.9524->0.9426 (98)    \n",
      "Epoch 435, NOT IMPROVED <total_loss: 0.0463> <bce_loss: 0.0274> <physics_loss: 0.0189> 0.9524->0.9357 (99)    \n",
      "Epoch 436, NOT IMPROVED <total_loss: 0.0452> <bce_loss: 0.0268> <physics_loss: 0.0184> 0.9524->0.9335 (100)    \n",
      "Epoch 437, NOT IMPROVED <total_loss: 0.0487> <bce_loss: 0.0303> <physics_loss: 0.0184> 0.9524->0.9333 (101)    \n",
      "Epoch 438, NOT IMPROVED <total_loss: 0.0485> <bce_loss: 0.0296> <physics_loss: 0.0189> 0.9524->0.9370 (102)    \n",
      "Epoch 439, NOT IMPROVED <total_loss: 0.0484> <bce_loss: 0.0305> <physics_loss: 0.0179> 0.9524->0.9353 (103)    \n",
      "Epoch 440, NOT IMPROVED <total_loss: 0.0440> <bce_loss: 0.0267> <physics_loss: 0.0173> 0.9524->0.9398 (104)    \n",
      "Epoch 441, NOT IMPROVED <total_loss: 0.0458> <bce_loss: 0.0282> <physics_loss: 0.0176> 0.9524->0.9319 (105)    \n",
      "Epoch 442, NOT IMPROVED <total_loss: 0.0463> <bce_loss: 0.0287> <physics_loss: 0.0176> 0.9524->0.9444 (106)    \n",
      "Epoch 443, NOT IMPROVED <total_loss: 0.0466> <bce_loss: 0.0285> <physics_loss: 0.0181> 0.9524->0.9463 (107)    \n",
      "Epoch 444, NOT IMPROVED <total_loss: 0.0455> <bce_loss: 0.0276> <physics_loss: 0.0179> 0.9524->0.9489 (108)    \n",
      "Epoch 445, NOT IMPROVED <total_loss: 0.0442> <bce_loss: 0.0261> <physics_loss: 0.0181> 0.9524->0.9333 (109)    \n",
      "Epoch 446, NOT IMPROVED <total_loss: 0.0479> <bce_loss: 0.0301> <physics_loss: 0.0179> 0.9524->0.9275 (110)    \n",
      "Epoch 447, NOT IMPROVED <total_loss: 0.0471> <bce_loss: 0.0290> <physics_loss: 0.0181> 0.9524->0.9369 (111)    \n",
      "Epoch 448, NOT IMPROVED <total_loss: 0.0469> <bce_loss: 0.0290> <physics_loss: 0.0179> 0.9524->0.9389 (112)    \n",
      "Epoch 449, NOT IMPROVED <total_loss: 0.0433> <bce_loss: 0.0262> <physics_loss: 0.0170> 0.9524->0.9333 (113)    \n",
      "Epoch 450, NOT IMPROVED <total_loss: 0.0436> <bce_loss: 0.0254> <physics_loss: 0.0181> 0.9524->0.9318 (114)    \n",
      "Epoch 451, NOT IMPROVED <total_loss: 0.0446> <bce_loss: 0.0262> <physics_loss: 0.0184> 0.9524->0.9363 (115)    \n",
      "Epoch 452, NOT IMPROVED <total_loss: 0.0477> <bce_loss: 0.0293> <physics_loss: 0.0184> 0.9524->0.9247 (116)    \n",
      "Epoch 453, NOT IMPROVED <total_loss: 0.0510> <bce_loss: 0.0323> <physics_loss: 0.0187> 0.9524->0.9306 (117)    \n",
      "Epoch 454, NOT IMPROVED <total_loss: 0.0537> <bce_loss: 0.0350> <physics_loss: 0.0187> 0.9524->0.9308 (118)    \n",
      "Epoch 455, NOT IMPROVED <total_loss: 0.0437> <bce_loss: 0.0262> <physics_loss: 0.0176> 0.9524->0.9335 (119)    \n",
      "Epoch 456, NOT IMPROVED <total_loss: 0.0429> <bce_loss: 0.0250> <physics_loss: 0.0179> 0.9524->0.9357 (120)    \n",
      "Epoch 457, NOT IMPROVED <total_loss: 0.0417> <bce_loss: 0.0243> <physics_loss: 0.0173> 0.9524->0.9455 (121)    \n",
      "Epoch 458, NOT IMPROVED <total_loss: 0.0435> <bce_loss: 0.0259> <physics_loss: 0.0176> 0.9524->0.9458 (122)    \n",
      "Epoch 459, NOT IMPROVED <total_loss: 0.0459> <bce_loss: 0.0281> <physics_loss: 0.0179> 0.9524->0.9389 (123)    \n",
      "Epoch 460, NOT IMPROVED <total_loss: 0.0423> <bce_loss: 0.0250> <physics_loss: 0.0173> 0.9524->0.9374 (124)    \n",
      "Epoch 461, NOT IMPROVED <total_loss: 0.0430> <bce_loss: 0.0246> <physics_loss: 0.0184> 0.9524->0.9387 (125)    \n",
      "Epoch 462, NOT IMPROVED <total_loss: 0.0435> <bce_loss: 0.0253> <physics_loss: 0.0181> 0.9524->0.9403 (126)    \n",
      "Epoch 463, NOT IMPROVED <total_loss: 0.0460> <bce_loss: 0.0270> <physics_loss: 0.0189> 0.9524->0.9410 (127)    \n",
      "Epoch 464, NOT IMPROVED <total_loss: 0.0435> <bce_loss: 0.0267> <physics_loss: 0.0168> 0.9524->0.9364 (128)    \n",
      "Epoch 465, NOT IMPROVED <total_loss: 0.0461> <bce_loss: 0.0280> <physics_loss: 0.0181> 0.9524->0.9315 (129)    \n",
      "Epoch 466, NOT IMPROVED <total_loss: 0.0458> <bce_loss: 0.0274> <physics_loss: 0.0184> 0.9524->0.9375 (130)    \n",
      "Epoch 467, NOT IMPROVED <total_loss: 0.0493> <bce_loss: 0.0309> <physics_loss: 0.0184> 0.9524->0.9405 (131)    \n",
      "Epoch 468, NOT IMPROVED <total_loss: 0.0521> <bce_loss: 0.0334> <physics_loss: 0.0187> 0.9524->0.9247 (132)    \n",
      "Epoch 469, NOT IMPROVED <total_loss: 0.0513> <bce_loss: 0.0321> <physics_loss: 0.0192> 0.9524->0.9292 (133)    \n",
      "Epoch 470, NOT IMPROVED <total_loss: 0.0535> <bce_loss: 0.0348> <physics_loss: 0.0187> 0.9524->0.9158 (134)    \n",
      "Epoch 471, NOT IMPROVED <total_loss: 0.0480> <bce_loss: 0.0302> <physics_loss: 0.0179> 0.9524->0.9297 (135)    \n",
      "Epoch 472, NOT IMPROVED <total_loss: 0.0473> <bce_loss: 0.0292> <physics_loss: 0.0181> 0.9524->0.9295 (136)    \n",
      "Epoch 473, NOT IMPROVED <total_loss: 0.0462> <bce_loss: 0.0286> <physics_loss: 0.0176> 0.9524->0.9377 (137)    \n",
      "Epoch 474, NOT IMPROVED <total_loss: 0.0462> <bce_loss: 0.0287> <physics_loss: 0.0176> 0.9524->0.9428 (138)    \n",
      "Epoch 475, NOT IMPROVED <total_loss: 0.0485> <bce_loss: 0.0293> <physics_loss: 0.0192> 0.9524->0.9365 (139)    \n",
      "Epoch 476, NOT IMPROVED <total_loss: 0.0459> <bce_loss: 0.0280> <physics_loss: 0.0179> 0.9524->0.9286 (140)    \n",
      "Epoch 477, NOT IMPROVED <total_loss: 0.0423> <bce_loss: 0.0242> <physics_loss: 0.0181> 0.9524->0.9380 (141)    \n",
      "Epoch 478, NOT IMPROVED <total_loss: 0.0450> <bce_loss: 0.0263> <physics_loss: 0.0187> 0.9524->0.9297 (142)    \n",
      "Epoch 479, NOT IMPROVED <total_loss: 0.0432> <bce_loss: 0.0253> <physics_loss: 0.0179> 0.9524->0.9433 (143)    \n",
      "Epoch 480, NOT IMPROVED <total_loss: 0.0459> <bce_loss: 0.0267> <physics_loss: 0.0192> 0.9524->0.9274 (144)    \n",
      "Epoch 481, NOT IMPROVED <total_loss: 0.0445> <bce_loss: 0.0266> <physics_loss: 0.0179> 0.9524->0.9412 (145)    \n",
      "Epoch 482, NOT IMPROVED <total_loss: 0.0460> <bce_loss: 0.0276> <physics_loss: 0.0184> 0.9524->0.9295 (146)    \n",
      "Epoch 483, NOT IMPROVED <total_loss: 0.0523> <bce_loss: 0.0339> <physics_loss: 0.0184> 0.9524->0.9327 (147)    \n",
      "Epoch 484, NOT IMPROVED <total_loss: 0.0467> <bce_loss: 0.0272> <physics_loss: 0.0195> 0.9524->0.9408 (148)    \n",
      "Epoch 485, NOT IMPROVED <total_loss: 0.0444> <bce_loss: 0.0258> <physics_loss: 0.0187> 0.9524->0.9433 (149)    \n",
      "Epoch 486, NOT IMPROVED <total_loss: 0.0467> <bce_loss: 0.0283> <physics_loss: 0.0184> 0.9524->0.9369 (150)    \n",
      "Epoch 487, NOT IMPROVED <total_loss: 0.0451> <bce_loss: 0.0264> <physics_loss: 0.0187> 0.9524->0.9405 (151)    \n",
      "Epoch 488, NOT IMPROVED <total_loss: 0.0436> <bce_loss: 0.0255> <physics_loss: 0.0181> 0.9524->0.9401 (152)    \n",
      "Epoch 489, NOT IMPROVED <total_loss: 0.0452> <bce_loss: 0.0263> <physics_loss: 0.0189> 0.9524->0.9381 (153)    \n",
      "Epoch 490, NOT IMPROVED <total_loss: 0.0446> <bce_loss: 0.0267> <physics_loss: 0.0179> 0.9524->0.9383 (154)    \n",
      "Epoch 491, NOT IMPROVED <total_loss: 0.0479> <bce_loss: 0.0295> <physics_loss: 0.0184> 0.9524->0.9421 (155)    \n",
      "Epoch 492, NOT IMPROVED <total_loss: 0.0453> <bce_loss: 0.0266> <physics_loss: 0.0187> 0.9524->0.9333 (156)    \n",
      "Epoch 493, NOT IMPROVED <total_loss: 0.0443> <bce_loss: 0.0253> <physics_loss: 0.0189> 0.9524->0.9292 (157)    \n",
      "Epoch 494, NOT IMPROVED <total_loss: 0.0428> <bce_loss: 0.0247> <physics_loss: 0.0181> 0.9524->0.9405 (158)    \n",
      "Epoch 495, NOT IMPROVED <total_loss: 0.0431> <bce_loss: 0.0250> <physics_loss: 0.0181> 0.9524->0.9343 (159)    \n",
      "Epoch 496, NOT IMPROVED <total_loss: 0.0430> <bce_loss: 0.0252> <physics_loss: 0.0179> 0.9524->0.9367 (160)    \n",
      "Epoch 497, NOT IMPROVED <total_loss: 0.0461> <bce_loss: 0.0282> <physics_loss: 0.0179> 0.9524->0.9355 (161)    \n",
      "Epoch 498, NOT IMPROVED <total_loss: 0.0442> <bce_loss: 0.0264> <physics_loss: 0.0179> 0.9524->0.9410 (162)    \n",
      "Epoch 499, NOT IMPROVED <total_loss: 0.0477> <bce_loss: 0.0293> <physics_loss: 0.0184> 0.9524->0.9393 (163)    \n",
      "Epoch 500, NOT IMPROVED <total_loss: 0.0452> <bce_loss: 0.0268> <physics_loss: 0.0184> 0.9524->0.9415 (164)    \n",
      "Epoch 501, NOT IMPROVED <total_loss: 0.0451> <bce_loss: 0.0270> <physics_loss: 0.0181> 0.9524->0.9385 (165)    \n",
      "Epoch 502, NOT IMPROVED <total_loss: 0.0439> <bce_loss: 0.0260> <physics_loss: 0.0179> 0.9524->0.9363 (166)    \n",
      "Epoch 503, NOT IMPROVED <total_loss: 0.0471> <bce_loss: 0.0298> <physics_loss: 0.0173> 0.9524->0.9345 (167)    \n",
      "Epoch 504, NOT IMPROVED <total_loss: 0.0445> <bce_loss: 0.0275> <physics_loss: 0.0170> 0.9524->0.9399 (168)    \n",
      "Epoch 505, NOT IMPROVED <total_loss: 0.0462> <bce_loss: 0.0283> <physics_loss: 0.0179> 0.9524->0.9415 (169)    \n",
      "Epoch 506, NOT IMPROVED <total_loss: 0.0484> <bce_loss: 0.0297> <physics_loss: 0.0187> 0.9524->0.9408 (170)    \n",
      "Epoch 507, NOT IMPROVED <total_loss: 0.0485> <bce_loss: 0.0290> <physics_loss: 0.0195> 0.9524->0.9413 (171)    \n",
      "Epoch 508, NOT IMPROVED <total_loss: 0.0452> <bce_loss: 0.0276> <physics_loss: 0.0176> 0.9524->0.9391 (172)    \n",
      "Epoch 509, NOT IMPROVED <total_loss: 0.0473> <bce_loss: 0.0283> <physics_loss: 0.0189> 0.9524->0.9365 (173)    \n",
      "Epoch 510, NOT IMPROVED <total_loss: 0.0462> <bce_loss: 0.0278> <physics_loss: 0.0184> 0.9524->0.9318 (174)    \n",
      "Epoch 511, NOT IMPROVED <total_loss: 0.0430> <bce_loss: 0.0246> <physics_loss: 0.0184> 0.9524->0.9272 (175)    \n",
      "Epoch 512, NOT IMPROVED <total_loss: 0.0454> <bce_loss: 0.0270> <physics_loss: 0.0184> 0.9524->0.9343 (176)    \n",
      "Epoch 513, NOT IMPROVED <total_loss: 0.0432> <bce_loss: 0.0259> <physics_loss: 0.0173> 0.9524->0.9351 (177)    \n",
      "Epoch 514, NOT IMPROVED <total_loss: 0.0437> <bce_loss: 0.0256> <physics_loss: 0.0181> 0.9524->0.9375 (178)    \n",
      "Epoch 515, NOT IMPROVED <total_loss: 0.0473> <bce_loss: 0.0298> <physics_loss: 0.0176> 0.9524->0.9391 (179)    \n",
      "Epoch 516, NOT IMPROVED <total_loss: 0.0478> <bce_loss: 0.0297> <physics_loss: 0.0181> 0.9524->0.9424 (180)    \n",
      "Epoch 517, NOT IMPROVED <total_loss: 0.0451> <bce_loss: 0.0270> <physics_loss: 0.0181> 0.9524->0.9408 (181)    \n",
      "Epoch 518, NOT IMPROVED <total_loss: 0.0479> <bce_loss: 0.0295> <physics_loss: 0.0184> 0.9524->0.9297 (182)    \n",
      "Epoch 519, NOT IMPROVED <total_loss: 0.0461> <bce_loss: 0.0283> <physics_loss: 0.0179> 0.9524->0.9351 (183)    \n",
      "Epoch 520, NOT IMPROVED <total_loss: 0.0497> <bce_loss: 0.0305> <physics_loss: 0.0192> 0.9524->0.9393 (184)    \n",
      "Epoch 521, NOT IMPROVED <total_loss: 0.0518> <bce_loss: 0.0342> <physics_loss: 0.0176> 0.9524->0.9291 (185)    \n",
      "Epoch 522, NOT IMPROVED <total_loss: 0.0474> <bce_loss: 0.0293> <physics_loss: 0.0181> 0.9524->0.9380 (186)    \n",
      "Epoch 523, NOT IMPROVED <total_loss: 0.0478> <bce_loss: 0.0297> <physics_loss: 0.0181> 0.9524->0.9377 (187)    \n",
      "Epoch 524, NOT IMPROVED <total_loss: 0.0531> <bce_loss: 0.0350> <physics_loss: 0.0181> 0.9524->0.9343 (188)    \n",
      "Epoch 525, NOT IMPROVED <total_loss: 0.0488> <bce_loss: 0.0312> <physics_loss: 0.0176> 0.9524->0.9321 (189)    \n",
      "Epoch 526, NOT IMPROVED <total_loss: 0.0473> <bce_loss: 0.0300> <physics_loss: 0.0173> 0.9524->0.9219 (190)    \n",
      "Epoch 527, NOT IMPROVED <total_loss: 0.0467> <bce_loss: 0.0281> <physics_loss: 0.0187> 0.9524->0.9315 (191)    \n",
      "Epoch 528, NOT IMPROVED <total_loss: 0.0465> <bce_loss: 0.0278> <physics_loss: 0.0187> 0.9524->0.9385 (192)    \n",
      "Epoch 529, NOT IMPROVED <total_loss: 0.0464> <bce_loss: 0.0278> <physics_loss: 0.0187> 0.9524->0.9361 (193)    \n",
      "Epoch 530, NOT IMPROVED <total_loss: 0.0494> <bce_loss: 0.0307> <physics_loss: 0.0187> 0.9524->0.9324 (194)    \n",
      "Epoch 531, NOT IMPROVED <total_loss: 0.0472> <bce_loss: 0.0285> <physics_loss: 0.0187> 0.9524->0.9449 (195)    \n",
      "Epoch 532, NOT IMPROVED <total_loss: 0.0504> <bce_loss: 0.0317> <physics_loss: 0.0187> 0.9524->0.9381 (196)    \n",
      "Epoch 533, NOT IMPROVED <total_loss: 0.0468> <bce_loss: 0.0286> <physics_loss: 0.0181> 0.9524->0.9412 (197)    \n",
      "Epoch 534, NOT IMPROVED <total_loss: 0.0466> <bce_loss: 0.0284> <physics_loss: 0.0181> 0.9524->0.9474 (198)    \n",
      "Epoch 535, NOT IMPROVED <total_loss: 0.0444> <bce_loss: 0.0258> <physics_loss: 0.0187> 0.9524->0.9389 (199)    \n",
      "Epoch 536, NOT IMPROVED <total_loss: 0.0428> <bce_loss: 0.0250> <physics_loss: 0.0179> 0.9524->0.9341 (200)    \n",
      "Epoch 537, NOT IMPROVED <total_loss: 0.0484> <bce_loss: 0.0305> <physics_loss: 0.0179> 0.9524->0.9353 (201)    \n"
     ]
    }
   ],
   "source": [
    "improvement_threshold = 10e-4  # Decimal precision for replacing the better model\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train[step:step + batch_size]\n",
    "        y_batch = y_train[step:step + batch_size]\n",
    "        loss_value = train_step(X_batch, y_batch, model, optimizer)  # Perform a training step\n",
    "\n",
    "    y_pred_val = model(X_val)  # Predict on the validation set\n",
    "    total_loss = pinn_loss((X_val, y_val), y_pred_val)  # Compute the total loss on the validation set\n",
    "    _, bce_loss, physics_loss = pinn_loss.getAllLosses()  # Get the individual loss components\n",
    "\n",
    "    f1_metric.update_state(y_val, y_pred_val)  # Update the F1 metric with the validation predictions\n",
    "    f1_score = f1_metric.result().numpy()  # Calculate the F1 score\n",
    "    f1_metric.reset_states()  # Reset the F1 metric state for the next epoch\n",
    "    # Check if the current F1 score is better than the previous best\n",
    "    if (tf.abs(f1_score - min_patience) < improvement_threshold) or (f1_score > min_patience):\n",
    "        print(\n",
    "            f\"\\rEpoch {epoch}, IMPROVED <total_loss: {total_loss.numpy():.4f}> <bce_loss: {bce_loss.numpy():.4f}> <physics_loss: {physics_loss.numpy():.4f}> {min_patience:.4f}->{f1_score:.4f} ({cont_patience + 1})     \")\n",
    "        cont_patience = 0  # Reset patience counter\n",
    "        if f1_score > min_patience:\n",
    "            best_model = model  # Update the best model\n",
    "            min_patience = f1_score\n",
    "    else:\n",
    "        cont_patience += 1  # Increment patience counter\n",
    "        print(\n",
    "            f\"\\rEpoch {epoch}, NOT IMPROVED <total_loss: {total_loss.numpy():.4f}> <bce_loss: {bce_loss.numpy():.4f}> <physics_loss: {physics_loss.numpy():.4f}> {min_patience:.4f}->{f1_score:.4f} ({cont_patience})    \")\n",
    "        if cont_patience > patience:  # Check if patience threshold is exceeded\n",
    "            break\n",
    "\n",
    "#Output interpretation:\n",
    "    #Epoch [EPOCH ID], [IMPROVED/NOT IMPROVED MESSAGE] <total_loss: [TOTAL_LOSS_VALUE]> <bce_loss: [FOCAL_LOSS_VALUE]> <physics_loss: [PHYSIC_LOSS_VALUE]> [BETTER_F1_SCORE]->[CURRENT_F1_SCORE] ([PATIENCE_COUNT])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29163d6335558dfa",
   "metadata": {},
   "source": [
    "### Testing stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d8ac109b0dc150e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:41:09.991971Z",
     "start_time": "2024-06-20T22:41:09.514090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)  # Predict on the test set\n",
    "y_pred = np.where(y_pred >= 0.5, 1, 0)  # Binarize the predictions based on a threshold of 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40b94576-1202-41d0-a890-242a4120dddb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:41:11.060341Z",
     "start_time": "2024-06-20T22:41:11.045629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3347   19]\n",
      " [  25  305]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99      3366\n",
      "         1.0       0.94      0.92      0.93       330\n",
      "\n",
      "    accuracy                           0.99      3696\n",
      "   macro avg       0.97      0.96      0.96      3696\n",
      "weighted avg       0.99      0.99      0.99      3696\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)  # Compute the confusion matrix\n",
    "cr = classification_report(y_test, y_pred)  # Generate the classification report\n",
    "\n",
    "print(cm)  # Print the confusion matrix\n",
    "print(cr)  # Print the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac46617f915b1b56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:41:12.644813Z",
     "start_time": "2024-06-20T22:41:12.638549Z"
    }
   },
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test,\n",
    "                                  y_pred).ravel()  # Extract true negatives, false positives, false negatives, and true positives from the confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b260f4aa9abaab9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:41:13.225647Z",
     "start_time": "2024-06-20T22:41:13.222122Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate Sensitivity\n",
    "def calculate_sensitivity(tp, fn):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    tp (int): Number of true positives.\n",
    "    fn (int): Number of false negatives.\n",
    "    \n",
    "    Returns:\n",
    "    float: The sensitivity (recall) value.\n",
    "    \"\"\"\n",
    "    return tp / (tp + fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ced8a1b-648c-41cd-945f-642bcd4da557",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:41:13.783598Z",
     "start_time": "2024-06-20T22:41:13.780537Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate Specificity\n",
    "def calculate_specificity(tn, fp):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    tn (int): Number of true negatives.\n",
    "    fp (int): Number of false positives.\n",
    "    \n",
    "    Returns:\n",
    "    float: The specificity value.\n",
    "    \"\"\"\n",
    "    return tn / (tn + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0b27f63-599f-408b-9582-2560ed7c997d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:41:14.064709Z",
     "start_time": "2024-06-20T22:41:14.056984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity or Recall: 0.9242424242424242\n",
      "Specificity: 0.9943553178847296\n",
      "Precision: 0.941358024691358\n",
      "F1 Score: 0.9327217125382261\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 score\n",
    "f1 = f1_s(y_test, y_pred)\n",
    "\n",
    "# Calculate Sensitivity (Recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "# Calculate Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Calculate Recall\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "# Calculate Precision\n",
    "precision = tp / (tp + fp)\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1 = 2 * ((recall * precision) / (recall + precision))\n",
    "\n",
    "# Print results\n",
    "print(\"Sensitivity or Recall:\", sensitivity)\n",
    "print(\"Specificity:\", specificity)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bc816d8-b8a1-4ddb-963f-fdcd207a6e24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T22:41:04.131190Z",
     "start_time": "2024-06-20T22:41:04.128659Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('../models/last_cnn_pinn_sag_overload.h5')  # Save the final trained model to the specified path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
