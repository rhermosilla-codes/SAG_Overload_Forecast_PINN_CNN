{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "930549a6ebc6d3a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T19:22:04.754801Z",
     "start_time": "2024-06-20T19:22:04.749637Z"
    }
   },
   "outputs": [],
   "source": [
    "#Libraries import\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Input, Conv2D, BatchNormalization, LeakyReLU, AveragePooling2D, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
    "from sklearn.metrics import f1_score as f1_s\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import gc\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b31df43ae9c9a",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d6e3ae2d9f12dee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T19:24:26.534139Z",
     "start_time": "2024-06-20T19:24:26.200282Z"
    }
   },
   "outputs": [],
   "source": [
    "#loading data from source\n",
    "data = np.load('../data/matrices.npz')\n",
    "X = data['matriz_a']  #X values \n",
    "y = data['matriz_b']  #y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ef9a418fa4d8f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T19:24:27.108545Z",
     "start_time": "2024-06-20T19:24:27.074994Z"
    }
   },
   "outputs": [],
   "source": [
    "#Split of data in a train, validation, and test sets. Observe the use of non-random to care for the time-series overlapping.\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.15, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bda955ddbd7bb1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T19:25:15.293774Z",
     "start_time": "2024-06-20T19:25:15.289785Z"
    }
   },
   "outputs": [],
   "source": [
    "#step to convert each set in valid tensors.\n",
    "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "X_val = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
    "y_val = tf.convert_to_tensor(y_val, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c323fd99122ecd9c",
   "metadata": {},
   "source": [
    "### PINNs definition section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3de10b7962f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of trainable parameters for each physics rule.\n",
    "# These lists contain trainable TensorFlow variables (l1 and l2) for 12 different physics rules.\n",
    "# Each variable is initialized to 1.0 and is of type float32.\n",
    "# The 'trainable=True' argument ensures that these variables will be optimized during the training process.\n",
    "# The 'name' argument assigns a unique name to each variable, using the format \"l1_i\" and \"l2_i\",\n",
    "# where 'i' is the index of the physics rule.\n",
    "l1 = [tf.Variable(initial_value=1.0, dtype=tf.float32, trainable=True, name=f\"l1_{i}\") for i in range(12)]\n",
    "l2 = [tf.Variable(initial_value=1.0, dtype=tf.float32, trainable=True, name=f\"l2_{i}\") for i in range(12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c6ce1965a6bb69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e9a50f260cca9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:45:00.791652Z",
     "start_time": "2024-06-20T17:45:00.785498Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the sigmoid activation function.\n",
    "# This function computes the sigmoid of the input tensor 'z'.\n",
    "# The sigmoid function is given by the formula: 1 / (1 + exp(-z)).\n",
    "# It maps any real-valued number into the range (0, 1).\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    z (tensor): Input tensor.\n",
    "    Returns:\n",
    "    tensor: The sigmoid of the input tensor.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + tf.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec46181da06b973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the normalize function.\n",
    "# This function scales the input pairs (alpha, beta) based on the maximum absolute value\n",
    "# among the elements in each pair. The purpose of this function is to normalize\n",
    "# the inputs to a common scale to improve numerical stability during computation.\n",
    "# The maximum absolute value between x and y is computed and used as the scaling factor.\n",
    "# Additionally, a small constant epsilon is added to avoid division by zero.\n",
    "def normalize(alpha, beta):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    alpha (tensor): First input tensor.\n",
    "    beta (tensor): Second input tensor.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the normalized tensors (x, y).\n",
    "    \"\"\"\n",
    "    max_value = tf.maximum(tf.abs(alpha), tf.abs(beta))\n",
    "    return alpha / (max_value + tf.keras.backend.epsilon()), beta / (max_value + tf.keras.backend.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e86d6b2ee3b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the slope_penalizer function.\n",
    "# This function penalizes the slopes of the input pairs (alpha, beta).\n",
    "# It first normalizes the input pairs using the normalize function.\n",
    "# Then, it computes the penalization condition based on the absolute values of the\n",
    "# trainable parameters (l1[i] and l2[i]) and applies the sigmoid function to these values.\n",
    "# The penalization condition is rounded to obtain binary values (0 or 1).\n",
    "# Finally, the penalization value is determined based on the penalization condition:\n",
    "# if the condition is greater than 0, the penalization value is set to 1.0, otherwise to 0.0.\n",
    "def slope_penalizer(alpha, beta, i):\n",
    "    x_norm, y_norm = normalize(alpha, beta)\n",
    "    penalization_condition = sigmoid(-tf.abs(l1[i]) * x_norm) * sigmoid(tf.abs(l2[i]) * y_norm)\n",
    "    penalization_condition = tf.round(penalization_condition)\n",
    "    penalization_value = tf.where(penalization_condition > 0, 1.0, 0.0)\n",
    "    return penalization_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "ff714e71-cde1-41a9-9c32-e8716d9fa016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicRules(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(PhysicRules, self).__init__()\n",
    "\n",
    "    # Define the forward pass (call method) for the Physics loss function.\n",
    "    # This method takes in the inputs alpha, beta, and y_pred.\n",
    "    # It applies a series of penalizer terms calculated by the slope_penalizer function\n",
    "    # for each pair (alpha[i], beta[i]).\n",
    "    # The maximum penalizer term across all pairs is computed and expanded to match the shape of y_pred.\n",
    "    # The y_pred tensor is binarized using a threshold of 0.5.\n",
    "    # The final loss is calculated as the product of the maximum penalizer term and the binarized y_pred.\n",
    "    def call(self, alpha, beta, y_pred):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        alpha (tensor): First input tensor.\n",
    "        beta (tensor): Second input tensor.\n",
    "        y_pred (tensor): Predicted output tensor.\n",
    "        \n",
    "        Returns:\n",
    "        tensor: The calculated physics-informed loss.\n",
    "        \"\"\"\n",
    "        penalized_terms = [slope_penalizer(alpha[i], beta[i], i) for i in range(len(alpha))]\n",
    "        reg_max = tf.reduce_max(tf.stack(penalized_terms, axis=0), axis=0)\n",
    "        reg_max = tf.expand_dims(reg_max, axis=-1)  # Expandir dims para hacer compatible con y_pred\n",
    "        y_pred = tf.where(y_pred >= 0.5, 1.0, 0.0)\n",
    "        loss_ = reg_max * y_pred\n",
    "        return loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5589f09664d1b970",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T19:42:47.198684Z",
     "start_time": "2024-06-20T19:42:47.194298Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the calculate_slope function.\n",
    "# This function calculates the mean slopes of the input tensor over time.\n",
    "# Inputs:\n",
    "# - inputs: A tensor of shape (batch_size, timesteps, features).\n",
    "# The function computes the differences between consecutive timesteps,\n",
    "# then divides these differences by the respective time intervals.\n",
    "# Finally, it averages the slopes over time for each feature and each sample in the batch.\n",
    "def calculate_slope(inputs):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    inputs (tensor): Input tensor of shape (batch_size, timesteps, features).\n",
    "    \n",
    "    Returns:\n",
    "    tensor: The mean slopes calculated from the input tensor over time.\n",
    "    \"\"\"\n",
    "    timesteps = tf.shape(inputs)[1]\n",
    "    diffs = inputs[:, 1:, :] - inputs[:, :-1, :]\n",
    "    time_intervals = tf.range(1, timesteps, dtype=tf.float32)\n",
    "    time_intervals = tf.reshape(time_intervals, (1, -1, 1))\n",
    "    slopes = diffs / time_intervals\n",
    "    mean_slopes = tf.reduce_mean(slopes, axis=1)\n",
    "    return mean_slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "b87e6c3a-ba17-411a-98fe-dd035c64d070",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:45:00.796925Z",
     "start_time": "2024-06-20T17:45:00.793225Z"
    }
   },
   "outputs": [],
   "source": [
    "physic_rules = PhysicRules()  # Create an instance of the PhysicRules class to use its methods and encapsulate its state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca90e80f-d9ba-4c16-af11-9fe0a3e6a2ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T19:59:13.720160Z",
     "start_time": "2024-06-20T19:59:13.711650Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the PhysicLoss class, which extends the tf.keras.losses.Loss class.\n",
    "# This class implements a custom loss function for a neural network incorporating\n",
    "# physics-informed constraints. The loss function combines a binary cross-entropy\n",
    "# loss with a physics-based penalty term.\n",
    "class PhysicLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, name=\"SAGLoss\", lambda_factor=1):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        name (str): Name of the loss function.\n",
    "        lambda_factor (float): Factor to scale the physics-based penalty term.\n",
    "        \"\"\"\n",
    "        super(PhysicLoss, self).__init__(name=name)\n",
    "        self.bce = BinaryCrossentropy()\n",
    "        self.calculate_slope = calculate_slope\n",
    "        self.lambda_factor = lambda_factor\n",
    "\n",
    "    # Calculate the gradients (slopes) of the input tensor.\n",
    "    # The gradients are used to determine the physics-based penalty terms.\n",
    "    def getGradients(self, inputs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        inputs (tensor): Input data for the model.\n",
    "        \n",
    "        Returns:\n",
    "        tuple: A tuple containing lists of alpha and beta gradients.\n",
    "        \"\"\"\n",
    "        gradients = self.calculate_slope(inputs)\n",
    "        alphas = [\n",
    "            gradients[:, 4],\n",
    "            gradients[:, 6],\n",
    "            gradients[:, 0],\n",
    "            gradients[:, 3],\n",
    "            gradients[:, 0],\n",
    "            gradients[:, 1],\n",
    "            gradients[:, 4],\n",
    "            gradients[:, 2]\n",
    "        ]\n",
    "        betas = [\n",
    "            gradients[:, 3],\n",
    "            gradients[:, 3],\n",
    "            gradients[:, 1],\n",
    "            gradients[:, 7],\n",
    "            gradients[:, 2],\n",
    "            gradients[:, 2],\n",
    "            gradients[:, 7],\n",
    "            gradients[:, 4]\n",
    "        ]\n",
    "        rules = [\n",
    "            [1, 1],  #up & down (default relation) 1\n",
    "            [1, 1],  #up & down 2\n",
    "            [-1, -1],  #down & up 3\n",
    "            [-1, -1],  #down & up 4\n",
    "            [-1, 1],  #down & down 5\n",
    "            [1, 1],  #up & down 6\n",
    "            [1, -1],  #up & up 7\n",
    "            [1, -1]  #up & up 8\n",
    "        ]\n",
    "        for i in range(len(rules)):\n",
    "            alphas[i] *= rules[i][0]\n",
    "            betas[i] *= rules[i][1]\n",
    "\n",
    "        return alphas, betas\n",
    "\n",
    "    # Calculate the physics-based loss.\n",
    "    # This loss penalizes violations of the physics-based constraints.\n",
    "    def getPhysicsLoss(self, inputs, y_pred):\n",
    "        \"\"\"\n",
    "         Parameters:\n",
    "         inputs (tensor): Input data for the model.\n",
    "         y_pred (tensor): Predicted output tensor.\n",
    "        \n",
    "         Returns:\n",
    "         tensor: The calculated physics-based loss.\n",
    "         \"\"\"\n",
    "        alphas, betas = self.getGradients(inputs)\n",
    "        penalized_max_loss_terms = physic_rules(alphas, betas, y_pred)\n",
    "        penalized_max_loss_terms = tf.reduce_mean(penalized_max_loss_terms)\n",
    "        return penalized_max_loss_terms\n",
    "\n",
    "    # Return all components of the loss for logging or analysis.\n",
    "    def getAllLosses(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        tuple: A tuple containing the total loss, focal loss, and physics loss.\n",
    "        \"\"\"\n",
    "        return self.total_loss, self.focal_loss, self.physic_loss\n",
    "\n",
    "    # Calculate the total loss, which is a combination of the focal loss and the physics-based loss.\n",
    "    def getTotalLoss(self, inputs, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        inputs (tensor): Input data for the model.\n",
    "        y_true (tensor): True labels corresponding to the input data.\n",
    "        y_pred (tensor): Predicted output tensor.\n",
    "        \n",
    "        Returns:\n",
    "        tensor: The total loss for this training step.\n",
    "        \"\"\"\n",
    "        focal_loss = self.bce(y_true, y_pred)\n",
    "        physic_loss = self.getPhysicsLoss(inputs, y_pred)\n",
    "        total_loss = focal_loss + self.lambda_factor * physic_loss\n",
    "        self.total_loss = total_loss\n",
    "        self.focal_loss = focal_loss\n",
    "        self.physic_loss = physic_loss\n",
    "        return total_loss\n",
    "\n",
    "    # Define the call method, which computes the total loss given the actual data and the predictions.\n",
    "    def call(self, actual_data, y_pred):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        actual_data (tuple): A tuple containing the input data and the true labels.\n",
    "        y_pred (tensor): Predicted output tensor.\n",
    "        \n",
    "        Returns:\n",
    "        tensor: The total loss for this training step.\n",
    "        \"\"\"\n",
    "        inputs, y_true = actual_data\n",
    "        total_loss = self.getTotalLoss(inputs, y_true, y_pred)\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5984a48b959a74c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T19:59:13.931484Z",
     "start_time": "2024-06-20T19:59:13.928181Z"
    }
   },
   "outputs": [],
   "source": [
    "pinn_loss = PhysicLoss(\"PhysicLoss\")  # Create an instance of the PhysicLoss class to use its custom loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aed1f260d19df6a",
   "metadata": {},
   "source": [
    "### Gramm matrices definition section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "773450894701a823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:45:00.819826Z",
     "start_time": "2024-06-20T17:45:00.814498Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the CalculateGramMatrix class, which extends the tf.keras.layers.Layer class.\n",
    "# This class computes the Gram matrix based on the input pairs of angles.\n",
    "class CalculateGramMatrix(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        pairs_n (list): List of pairs of indices used to calculate angular differences.\n",
    "        \"\"\"\n",
    "        acceptable_kwargs = {k: v for k, v in kwargs.items() if\n",
    "                             k in ['name', 'trainable', 'dtype', 'dynamic', 'input_shape']}\n",
    "        super(CalculateGramMatrix, self).__init__(**acceptable_kwargs)\n",
    "        self.pairs_n = kwargs.get('pairs_n',\n",
    "                                  [[4, 3], [6, 4], [6, 3], [3, 5], [0, 1], [3, 7], [5, 7], [0, 2], [1, 2], [4, 7],\n",
    "                                   [2, 4]])\n",
    "\n",
    "    # Define the forward pass (call method) for the CalculateGramMatrix layer.\n",
    "    # This method takes in the inputs and computes the Gram matrix based on angular differences.\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        inputs (tensor): Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "        tensor: The Gram matrix calculated from the angular differences.\n",
    "        \"\"\"\n",
    "        angles = tf.math.acos(inputs)\n",
    "\n",
    "        matrix = []\n",
    "        for i, j in self.pairs_n:\n",
    "            angles_j1 = angles[:, :, i]  # Angles of the first feature in the pair.\n",
    "            angles_j2 = angles[:, :, j]  # Angles of the second feature in the pair.\n",
    "\n",
    "            #expand the matrices to allow the next add operations\n",
    "            angles_j1_expanded = tf.expand_dims(angles_j1, axis=2)\n",
    "            angles_j2_expanded = tf.expand_dims(angles_j2, axis=1)\n",
    "\n",
    "            # Calculate the angular difference matrices (30x30)\n",
    "            angular_difference_matrix = angles_j1_expanded - angles_j2_expanded\n",
    "            matrix.append(angular_difference_matrix)\n",
    "        m = tf.stack(matrix, axis=-1)\n",
    "        return m\n",
    "\n",
    "    # Define the get_config method to serialize the layer configuration.\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        dict: Configuration dictionary for serializing the layer.\n",
    "        \"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update({'pairs_n': self.pairs_n})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23fc095565c78a1",
   "metadata": {},
   "source": [
    "### Temporal filter definition section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "5e66467ed8a39f1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:45:00.826513Z",
     "start_time": "2024-06-20T17:45:00.821116Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the CustomTemporalFilter class, which extends the tf.keras.layers.Layer class.\n",
    "# This class applies a custom temporal filter to the input tensor.\n",
    "class CustomTemporalFilter(Layer):\n",
    "    def __init__(self, filter_size, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        filter_size (int): Size of the filter to be applied.\n",
    "        \"\"\"\n",
    "        super(CustomTemporalFilter, self).__init__(**kwargs)\n",
    "        self.filter_size = filter_size\n",
    "\n",
    "    # Define the get_config method to serialize the layer configuration.\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        dict: Configuration dictionary for serializing the layer.\n",
    "        \"\"\"\n",
    "        config = super(CustomTemporalFilter, self).get_config()\n",
    "        config.update({\n",
    "            \"filter_size\": self.filter_size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    # Build the layer by creating an filter that decreases from the bottom-right\n",
    "    # corner to the top-left corner.\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        input_shape (tensor): Shape of the input tensor.\n",
    "        \"\"\"\n",
    "        # Create a 2D matrix where each element is the value of its normalized index.\n",
    "        # This creates a gradient that decreases towards the top-left corner.\n",
    "        x = tf.linspace(1.0, 0.0, self.filter_size)\n",
    "        y = tf.linspace(1.0, 0.0, self.filter_size)\n",
    "        X, Y = tf.meshgrid(x, y)\n",
    "        self.filter = 1.0 - ((X + Y) / 2.0)  # Normalize to have values from 0 to 1\n",
    "        self.filter = tf.reshape(self.filter, (1, self.filter_size, self.filter_size, 1))\n",
    "        self.filter = tf.cast(self.filter, dtype='float32')\n",
    "\n",
    "    # Define the forward pass (call method) for the CustomTemporalFilter layer.\n",
    "    # This method applies the custom temporal filter to the input tensor.\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        inputs (tensor): Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "        tensor: The input tensor after applying the custom temporal filter.\n",
    "        \"\"\"\n",
    "        # Adjust the filter to match the batch size and number of channels of the inputs.\n",
    "        filter_broadcasted = tf.tile(self.filter, [tf.shape(inputs)[0], 1, 1, tf.shape(inputs)[-1]])\n",
    "\n",
    "        # Apply the filter to the inputs.\n",
    "        return inputs * filter_broadcasted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98df7880786d109",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "50aa5dd6361d7fb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:45:00.838637Z",
     "start_time": "2024-06-20T17:45:00.833321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the function to create the model.\n",
    "# This function builds a neural network model with specified numbers of convolutional and dense layers.\n",
    "# The model includes a CalculateGramMatrix layer, a CustomTemporalFilter layer,\n",
    "# and dynamically adds convolutional and dense layers based on the given parameters.\n",
    "def create_model(num_conv_layers, num_dense_layers, num_neurons, dropout_rate, conv_filters):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    num_conv_layers (int): Number of convolutional layers to add.\n",
    "    num_dense_layers (int): Number of dense layers to add.\n",
    "    num_neurons (int): Number of neurons in the first dense layer.\n",
    "    dropout_rate (float): Dropout rate to apply after each dense layer.\n",
    "    conv_filters (int): Number of filters in the first convolutional layer.\n",
    "    \n",
    "    Returns:\n",
    "    tf.keras.Model: The constructed neural network model.\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(30, 8), name=\"Input\")\n",
    "    m = CalculateGramMatrix(name=\"Gram_converter\")(input_layer)\n",
    "    m = CustomTemporalFilter(filter_size=30, name=\"Temporal_filter\")(m)\n",
    "\n",
    "    # Add convolutional layers dynamically\n",
    "    for i in range(num_conv_layers):\n",
    "        print(f'conv_filters: {conv_filters * (2 ** i)}')\n",
    "        m = Conv2D(filters=conv_filters * (2 ** i), kernel_size=(3, 3), use_bias=False, kernel_initializer='he_normal')(\n",
    "            m)\n",
    "        m = BatchNormalization()(m)\n",
    "        m = LeakyReLU(alpha=0.01)(m)\n",
    "        m = AveragePooling2D(pool_size=(2, 2))(m)\n",
    "\n",
    "    c = Flatten(name=\"Flattened_after_full\")(m)\n",
    "\n",
    "    # Add dense layers dynamically\n",
    "    for j in range(num_dense_layers):\n",
    "        print(f'num_neurons: {num_neurons // (2 ** j)}')\n",
    "        c = Dense(num_neurons // (2 ** j), activation='relu', kernel_initializer='he_normal')(c)\n",
    "        c = Dropout(dropout_rate)(c)\n",
    "        c = BatchNormalization()(c)\n",
    "    output_layer = Dense(1, activation='sigmoid', name=\"Output\")(c)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "c6160afd6d36cae8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:45:00.843257Z",
     "start_time": "2024-06-20T17:45:00.840107Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the function to delete the model and clear the session.\n",
    "# This function clears the current TensorFlow session, deletes the model and optimizer variables,\n",
    "# and performs garbage collection to free up memory.\n",
    "def del_model():\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    None\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "    try:\n",
    "        del model\n",
    "        del optimizer\n",
    "    except:\n",
    "        None\n",
    "    for i in range(15):\n",
    "        gc.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "df8c93bf695b599f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:45:00.849098Z",
     "start_time": "2024-06-20T17:45:00.844693Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the F1Score class, which extends the tf.keras.metrics.Metric class.\n",
    "# This class calculates the F1 score, which is the harmonic mean of precision and recall.\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        \"\"\"        \n",
    "        Parameters:\n",
    "        name (str): Name of the metric.\n",
    "        kwargs (dict): Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = Precision()\n",
    "        self.recall = Recall()\n",
    "\n",
    "    # Update the state of the metric with the true and predicted values.\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"        \n",
    "        Parameters:\n",
    "        y_true (tensor): True labels.\n",
    "        y_pred (tensor): Predicted labels.\n",
    "        sample_weight (tensor, optional): Optional weighting of each example.\n",
    "        \n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    # Compute the result of the metric.\n",
    "    def result(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        tensor: The computed F1 score.\n",
    "        \"\"\"\n",
    "        precision = self.precision.result()\n",
    "        recall = self.recall.result()\n",
    "        return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n",
    "\n",
    "    # Reset the states of the precision and recall metrics.\n",
    "    def reset_states(self):\n",
    "        \"\"\"  \n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "446b6348-62fc-4934-8876-3f5f6e07a293",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T20:02:29.306820Z",
     "start_time": "2024-06-20T20:02:29.302319Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform a single training step.\n",
    "# This function computes the model predictions, calculates the custom PINN loss,\n",
    "# computes gradients, and applies these gradients to update the model's weights.\n",
    "@tf.function\n",
    "def train_step(inputs, y_true, model, optimizer):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    inputs (tensor): Input data for the model.\n",
    "    y_true (tensor): True labels corresponding to the input data.\n",
    "    model (tf.keras.Model): The model to be trained.\n",
    "    optimizer (tf.keras.optimizers.Optimizer): The optimizer to use for updating the model's weights.\n",
    "    \n",
    "    Returns:\n",
    "    tensor: The total loss for this training step.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(inputs, training=True)\n",
    "        total_loss = pinn_loss((inputs, y_true), y_pred)  # Compute the custom PINN loss\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)  # Calculate gradients\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))  # Apply gradients to update model weights\n",
    "    return total_loss  # Return the total loss for this training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "1100e1e7-2481-468f-b4eb-048825e72b8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:45:00.867346Z",
     "start_time": "2024-06-20T17:45:00.863935Z"
    }
   },
   "outputs": [],
   "source": [
    "# These parameters are the result of a grid search.\n",
    "# The grid search was conducted to find the optimal hyperparameters\n",
    "# for the neural network model.\n",
    "params = {\n",
    "    'num_conv_layers': 2,\n",
    "    'num_dense_layers': 3,\n",
    "    'num_neurons': 1024,\n",
    "    'dropout_rate': 0.3,\n",
    "    'lambda_factor': 1.0,  # Put in 0.0 to deactivate PINN loss calculation\n",
    "    'conv_filters': 64  # Different base numbers of filters for the convolutional layers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a215f847-7960-48b0-aa7b-1d3741ba5fa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:45:01.106021Z",
     "start_time": "2024-06-20T17:45:00.901256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_filters: 64\n",
      "conv_filters: 128\n",
      "num_neurons: 1024\n",
      "num_neurons: 512\n",
      "num_neurons: 256\n"
     ]
    }
   ],
   "source": [
    "model = create_model(\n",
    "    num_conv_layers=params['num_conv_layers'],\n",
    "    num_dense_layers=params['num_dense_layers'],\n",
    "    num_neurons=params['num_neurons'],\n",
    "    dropout_rate=params['dropout_rate'],\n",
    "    conv_filters=params['conv_filters']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "cb4e5f51-ea9a-4c6e-95a3-896887c5927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the evaluation metrics and loss function.\n",
    "# These metrics and loss function will be used to assess the performance of the model.\n",
    "\n",
    "precision_metric = Precision()  # Precision metric to measure the proportion of true positives among all positive predictions\n",
    "recall_metric = Recall()  # Recall metric to measure the proportion of true positives among all actual positives\n",
    "bce = BinaryFocalCrossentropy()  # Binary Focal Crossentropy loss function to handle class imbalance\n",
    "f1_metric = F1Score()  # F1 Score metric to combine precision and recall into a single measure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7421c857f3581d7",
   "metadata": {},
   "source": [
    "### Training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "7e4c67d5-c4be-4751-9d02-bb0acc86b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training parameters.\n",
    "patience = 200  # Number of epochs with no improvement after which training will be stopped\n",
    "epochs = 5000  # Total number of epochs to train the model\n",
    "cont_patience = 0  # Counter to keep track of epochs with no improvement\n",
    "min_patience = 0  # Variable to store the minimum patience value observed\n",
    "best_model = None  # Variable to store the best model observed during training\n",
    "batch_size = 2048  # Size of the mini-batches used during training\n",
    "del_model()  # Clear any existing models and free up memory\n",
    "lambda_factor = params['lambda_factor']  # Retrieve the lambda factor from the parameters dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "673f2b0c-5223-4128-b16a-c431c91a31da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:45:01.137504Z",
     "start_time": "2024-06-20T17:45:01.112186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the optimizer and build it with the model's trainable variables.\n",
    "optimizer = tf.keras.optimizers.Adam()  # Use Adam optimizer for training\n",
    "optimizer.build(model.trainable_variables)  # Build the optimizer with the trainable variables of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "c18e95b8-a43c-464b-ac14-90462e115914",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T18:09:51.675609Z",
     "start_time": "2024-06-20T17:45:01.152403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1260, ULTIMO   <total_loss: 0.16821610927581787> <bce_loss: 0.08244771510362625> <physics_loss: 0.08576840162277222> 0.95->0.9402984380722046 (201)    "
     ]
    }
   ],
   "source": [
    "precision = 10e-4  # Decimal precision for replacing the better model\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train[step:step + batch_size]\n",
    "        y_batch = y_train[step:step + batch_size]\n",
    "        loss_value = train_step(X_batch, y_batch, model, optimizer)  # Perform a training step\n",
    "\n",
    "    y_pred_val = model(X_val)  # Predict on the validation set\n",
    "    total_loss = pinn_loss((X_val, y_val), y_pred_val)  # Compute the total loss on the validation set\n",
    "    _, bce_loss, physics_loss = pinn_loss.getAllLosses()  # Get the individual loss components\n",
    "\n",
    "    f1_metric.update_state(y_val, y_pred_val)  # Update the F1 metric with the validation predictions\n",
    "    f1_score = f1_metric.result().numpy()  # Calculate the F1 score\n",
    "    f1_metric.reset_states()  # Reset the F1 metric state for the next epoch\n",
    "\n",
    "    # Check if the current F1 score is better than the previous best\n",
    "    if (tf.abs(f1_score - min_patience) < precision) or (f1_score > min_patience):\n",
    "        sys.stdout.write(\n",
    "            f\"\\rEpoch {epoch}, IMPROVED <total_loss: {total_loss}> <bce_loss: {bce_loss}> <physics_loss: {physics_loss}> {min_patience}->{f1_score} ({cont_patience + 1})     \")\n",
    "        cont_patience = 0  # Reset patience counter\n",
    "        sys.stdout.flush()\n",
    "        best_model = model  # Update the best model\n",
    "        min_patience = tf.floor(f1_score * 100) / 100  # Update the minimum patience value\n",
    "        increase = True\n",
    "    else:\n",
    "        cont_patience += 1  # Increment patience counter\n",
    "        sys.stdout.write(\n",
    "            f\"\\rEpoch {epoch}, LAST IMPROVEMENT <total_loss: {total_loss}> <bce_loss: {bce_loss}> <physics_loss: {physics_loss}> {min_patience}->{f1_score} ({cont_patience})    \")\n",
    "        sys.stdout.flush()\n",
    "        if cont_patience > patience:  # Check if patience threshold is exceeded\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29163d6335558dfa",
   "metadata": {},
   "source": [
    "### Testing stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "9d8ac109b0dc150e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T18:09:52.163962Z",
     "start_time": "2024-06-20T18:09:51.677065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)  # Predict on the test set\n",
    "y_pred = np.where(y_pred >= 0.5, 1, 0)  # Binarize the predictions based on a threshold of 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "40b94576-1202-41d0-a890-242a4120dddb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T18:09:52.183226Z",
     "start_time": "2024-06-20T18:09:52.170749Z"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)  # Compute the confusion matrix\n",
    "cr = classification_report(y_test, y_pred)  # Generate the classification report\n",
    "\n",
    "print(cm)  # Print the confusion matrix\n",
    "print(cr)  # Print the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac46617f915b1b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test,\n",
    "                                  y_pred).ravel()  # Extract true negatives, false positives, false negatives, and true positives from the confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b260f4aa9abaab9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T18:09:52.197713Z",
     "start_time": "2024-06-20T18:09:52.194860Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate Sensitivity\n",
    "def calculate_sensitivity(tp, fn):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    tp (int): Number of true positives.\n",
    "    fn (int): Number of false negatives.\n",
    "    \n",
    "    Returns:\n",
    "    float: The sensitivity (recall) value.\n",
    "    \"\"\"\n",
    "    return tp / (tp + fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "4ced8a1b-648c-41cd-945f-642bcd4da557",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate Specificity\n",
    "def calculate_specificity(tn, fp):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    tn (int): Number of true negatives.\n",
    "    fp (int): Number of false positives.\n",
    "    \n",
    "    Returns:\n",
    "    float: The specificity value.\n",
    "    \"\"\"\n",
    "    return tn / (tn + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "a0b27f63-599f-408b-9582-2560ed7c997d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T18:09:52.216865Z",
     "start_time": "2024-06-20T18:09:52.210664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity o Recall: 0.9181818181818182\n",
      "Specificity: 0.995840760546643\n",
      "Precision 0.9558359621451105\n",
      "F1 Score: 0.9366306027820711\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 score\n",
    "f1 = f1_s(y_test, y_pred)\n",
    "\n",
    "# Calculate Sensitivity (Recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "# Calculate Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Calculate Recall\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "# Calculate Precision\n",
    "precision = tp / (tp + fp)\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1 = 2 * ((recall * precision) / (recall + precision))\n",
    "\n",
    "# Print results\n",
    "print(\"Sensitivity or Recall:\", sensitivity)\n",
    "print(\"Specificity:\", specificity)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "0bc816d8-b8a1-4ddb-963f-fdcd207a6e24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T18:09:52.350041Z",
     "start_time": "2024-06-20T18:09:52.218199Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('../models/last_cnn_pinn_sag_overload.h5')  # Save the final trained model to the specified path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
